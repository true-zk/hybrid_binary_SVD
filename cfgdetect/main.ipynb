{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### main.ipynb\n",
    "- load data\n",
    "- train model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import gendata\n",
    "import torch\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing, TopKPooling\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of dataset:1920,         len of train:1344,         len of validate:192,         len of test:384\n",
      "good in testset:183;;bad in testset:201\n",
      "data load time:  138.4431014060974\n"
     ]
    }
   ],
   "source": [
    "# start time log\n",
    "time_start = time.time()\n",
    "\n",
    "# load data -> Dataset\n",
    "data_path_good = r\"D:\\Desktop\\hybrid-SVD\\datasrc\\CWE23\\CWE23_good\\dot\"\n",
    "data_path_bad = r\"D:\\Desktop\\hybrid-SVD\\datasrc\\CWE23\\CWE23_bad\\dot\"\n",
    "\n",
    "Dataset = gendata.gendata(data_path_good)\n",
    "Dataset = Dataset + gendata.gendata(data_path_bad)\n",
    "\n",
    "random.shuffle(Dataset)\n",
    "\n",
    "lenDataset = len(Dataset)\n",
    "len_Trainset = int(0.7 * lenDataset)\n",
    "len_Testset = int(0.2 * lenDataset)\n",
    "len_Validateset = lenDataset - len_Testset - len_Trainset\n",
    "Trainset = Dataset[:len_Trainset]\n",
    "Testset = Dataset[len_Trainset :  len_Trainset+len_Testset]\n",
    "Validset = Dataset[len_Trainset+len_Testset:]\n",
    "print(f\"len of dataset:{lenDataset}, \\\n",
    "        len of train:{len_Trainset}, \\\n",
    "        len of validate:{len_Validateset}, \\\n",
    "        len of test:{len_Testset}\")\n",
    "\n",
    "# dataset -> loader\n",
    "train_loader = DataLoader(Trainset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(Testset, batch_size=32, shuffle=True)\n",
    "good = 0\n",
    "bad = 0\n",
    "for test in Testset:\n",
    "    if test[\"y\"] == 1:\n",
    "        good += 1\n",
    "    else:\n",
    "        bad += 1\n",
    "print(f\"good in testset:{good};;bad in testset:{bad}\")\n",
    "\n",
    "# fin load ; print time span\n",
    "time_fin_load = time.time()\n",
    "print(\"data load time: \", time_fin_load-time_start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# GCN layer\n",
    "# cite: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__(aggr='add', **kwargs)  # \"Add\" aggregation (Step 5).\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.lin = Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x[N, in_channels]\n",
    "        # edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        x = self.lin(x)\n",
    "        row, col = edge_index\n",
    "\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        return  self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        # Step 6: Return new node embeddings.\n",
    "        return aggr_out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Whole NN model\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(256, 256)\n",
    "        # self.conv1 = GraphConv(128, 128)\n",
    "        self.pool1 = TopKPooling(256, ratio=0.8)\n",
    "        self.conv2 = GCNConv(256, 256)\n",
    "        # self.conv2 = GraphConv(128, 128)\n",
    "        self.pool2 = TopKPooling(256, ratio=0.8)\n",
    "        self.conv3 = GCNConv(256, 256)\n",
    "        # self.conv3 = GraphConv(128, 128)\n",
    "        self.pool3 = TopKPooling(256, ratio=0.8)\n",
    "        self.conv4 = GCNConv(256, 256)\n",
    "        self.pool4 = TopKPooling(256, ratio=0.8)\n",
    "        self.conv5 = GCNConv(256, 256)\n",
    "        self.pool5 = TopKPooling(256, ratio=0.8)\n",
    "\n",
    "        self.convAtt1 = torch.nn.Conv1d(in_channels=512, out_channels=64, kernel_size=1, stride=2)\n",
    "        self.poolAtt1 = torch.nn.MaxPool1d(kernel_size=1, stride=2)\n",
    "        self.convAtt2 = torch.nn.Conv1d(64, 16, kernel_size=1, stride=2)\n",
    "        self.poolAtt2 = torch.nn.MaxPool1d(kernel_size=1, stride=2)\n",
    "        self.convAtt3 = torch.nn.Conv1d(16, 2, kernel_size=1, stride=2)\n",
    "        self.poolAtt3 = torch.nn.MaxPool1d(kernel_size=1, stride=2)\n",
    "\n",
    "        self.convAtt4 = torch.nn.Conv1d(2, 16, kernel_size=1, stride=2)\n",
    "        self.poolAtt4 = torch.nn.MaxPool1d(kernel_size=1, stride=2)\n",
    "        self.convAtt5 = torch.nn.Conv1d(16, 64, kernel_size=1, stride=2)\n",
    "        self.poolAtt5 = torch.nn.MaxPool1d(kernel_size=1, stride=2)\n",
    "        self.convAtt6 = torch.nn.Conv1d(64, 512, kernel_size=1, stride=2)\n",
    "        self.poolAtt6 = torch.nn.MaxPool1d(kernel_size=1, stride=2)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(512, 128)\n",
    "        self.lin2 = torch.nn.Linear(128, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, 2)\n",
    "\n",
    "        self.readout = Seq(Linear(128, 64),\n",
    "                           ReLU(),\n",
    "                           Linear(64, 2))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool4(x, edge_index, None, batch)\n",
    "        x4 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool5(x, edge_index, None, batch)\n",
    "        x5 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 + x2 + x3 + x4 + x5\n",
    "\n",
    "        x = 1/5 * x\n",
    "\n",
    "        sx = x\n",
    "\n",
    "        x = x.unsqueeze(dim=2)\n",
    "        # attentionå±‚\n",
    "        x = F.relu(self.convAtt1(x))\n",
    "        x = self.poolAtt1(x)\n",
    "        x = F.relu(self.convAtt2(x))\n",
    "        x = self.poolAtt2(x)\n",
    "        x = F.relu(self.convAtt3(x))\n",
    "        x = self.poolAtt3(x)\n",
    "\n",
    "        x = F.relu(self.convAtt4(x))\n",
    "        x = self.poolAtt4(x)\n",
    "        x = F.relu(self.convAtt5(x))\n",
    "        x = self.poolAtt5(x)\n",
    "        x = F.relu(self.convAtt6(x))\n",
    "        x = self.poolAtt6(x)\n",
    "\n",
    "        x = x.squeeze()\n",
    "\n",
    "        x = (x + 1) * sx\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        output = model(x, edge_index, batch)\n",
    "        loss = F.nll_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(Trainset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def test(t_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    pred_good = 0\n",
    "    pred_bad = 0\n",
    "    for data in t_loader:\n",
    "        data = data.to(device)\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        preds = model(x, edge_index, batch).max(dim=1)[1]\n",
    "        correct += preds.eq(data.y).sum().item()\n",
    "        for i in range(len(preds)):\n",
    "            if preds[i] == 0 and data.y[i] == 0:\n",
    "                tp += 1\n",
    "                pred_bad += 1\n",
    "            elif preds[i] == 0 and data.y[i] == 1:\n",
    "                fp += 1\n",
    "                pred_bad += 1\n",
    "            elif preds[i] == 1 and data.y[i] == 1:\n",
    "                tn += 1\n",
    "                pred_good += 1\n",
    "            elif preds[i] == 1 and data.y[i] == 0:\n",
    "                fn += 1\n",
    "                pred_good += 1\n",
    "    acc = (correct/len(t_loader.dataset))\n",
    "    return round(acc, 6), tp, fp, tn, fn, pred_good, pred_bad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.68891, Train Acc: 0.72693,Test Acc: 0.69010, TP: 124, FP: 42, TN: 141, FN: 77, Pred_good: 0218, Pred_bad: 0166\n",
      "Epoch: 001, Loss: 0.60946, Train Acc: 0.78274,Test Acc: 0.76042, TP: 119, FP: 10, TN: 173, FN: 82, Pred_good: 0255, Pred_bad: 0129\n",
      "Epoch: 002, Loss: 0.50365, Train Acc: 0.75670,Test Acc: 0.75781, TP: 183, FP: 75, TN: 108, FN: 18, Pred_good: 0126, Pred_bad: 0258\n",
      "Epoch: 003, Loss: 0.45540, Train Acc: 0.83259,Test Acc: 0.83073, TP: 201, FP: 65, TN: 118, FN: 00, Pred_good: 0118, Pred_bad: 0266\n",
      "Epoch: 004, Loss: 0.39744, Train Acc: 0.82366,Test Acc: 0.83333, TP: 198, FP: 61, TN: 122, FN: 03, Pred_good: 0125, Pred_bad: 0259\n",
      "Epoch: 005, Loss: 0.36907, Train Acc: 0.83631,Test Acc: 0.83333, TP: 201, FP: 64, TN: 119, FN: 00, Pred_good: 0119, Pred_bad: 0265\n",
      "Epoch: 006, Loss: 0.36320, Train Acc: 0.83929,Test Acc: 0.84635, TP: 198, FP: 56, TN: 127, FN: 03, Pred_good: 0130, Pred_bad: 0254\n",
      "Epoch: 007, Loss: 0.48416, Train Acc: 0.81920,Test Acc: 0.81771, TP: 182, FP: 51, TN: 132, FN: 19, Pred_good: 0151, Pred_bad: 0233\n",
      "Epoch: 008, Loss: 0.38508, Train Acc: 0.85491,Test Acc: 0.81510, TP: 158, FP: 28, TN: 155, FN: 43, Pred_good: 0198, Pred_bad: 0186\n",
      "Epoch: 009, Loss: 0.33987, Train Acc: 0.78051,Test Acc: 0.77604, TP: 182, FP: 67, TN: 116, FN: 19, Pred_good: 0135, Pred_bad: 0249\n",
      "Epoch: 010, Loss: 0.34859, Train Acc: 0.86533,Test Acc: 0.84896, TP: 165, FP: 22, TN: 161, FN: 36, Pred_good: 0197, Pred_bad: 0187\n",
      "Epoch: 011, Loss: 0.31830, Train Acc: 0.88467,Test Acc: 0.86198, TP: 187, FP: 39, TN: 144, FN: 14, Pred_good: 0158, Pred_bad: 0226\n",
      "Epoch: 012, Loss: 0.26681, Train Acc: 0.89732,Test Acc: 0.89062, TP: 201, FP: 42, TN: 141, FN: 00, Pred_good: 0141, Pred_bad: 0243\n",
      "Epoch: 013, Loss: 0.20936, Train Acc: 0.90997,Test Acc: 0.86719, TP: 164, FP: 14, TN: 169, FN: 37, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 014, Loss: 0.18988, Train Acc: 0.91964,Test Acc: 0.86979, TP: 166, FP: 15, TN: 168, FN: 35, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 015, Loss: 0.18291, Train Acc: 0.91146,Test Acc: 0.91667, TP: 201, FP: 32, TN: 151, FN: 00, Pred_good: 0151, Pred_bad: 0233\n",
      "Epoch: 016, Loss: 0.20686, Train Acc: 0.90923,Test Acc: 0.89062, TP: 184, FP: 25, TN: 158, FN: 17, Pred_good: 0175, Pred_bad: 0209\n",
      "Epoch: 017, Loss: 0.17745, Train Acc: 0.91592,Test Acc: 0.91667, TP: 201, FP: 32, TN: 151, FN: 00, Pred_good: 0151, Pred_bad: 0233\n",
      "Epoch: 018, Loss: 0.25536, Train Acc: 0.90476,Test Acc: 0.88542, TP: 176, FP: 19, TN: 164, FN: 25, Pred_good: 0189, Pred_bad: 0195\n",
      "Epoch: 019, Loss: 0.23155, Train Acc: 0.90179,Test Acc: 0.90885, TP: 193, FP: 27, TN: 156, FN: 08, Pred_good: 0164, Pred_bad: 0220\n",
      "Epoch: 020, Loss: 0.24035, Train Acc: 0.92113,Test Acc: 0.91927, TP: 197, FP: 27, TN: 156, FN: 04, Pred_good: 0160, Pred_bad: 0224\n",
      "Epoch: 021, Loss: 0.21050, Train Acc: 0.91220,Test Acc: 0.91146, TP: 195, FP: 28, TN: 155, FN: 06, Pred_good: 0161, Pred_bad: 0223\n",
      "Epoch: 022, Loss: 0.18403, Train Acc: 0.90476,Test Acc: 0.90104, TP: 193, FP: 30, TN: 153, FN: 08, Pred_good: 0161, Pred_bad: 0223\n",
      "Epoch: 023, Loss: 0.16107, Train Acc: 0.90327,Test Acc: 0.90625, TP: 201, FP: 36, TN: 147, FN: 00, Pred_good: 0147, Pred_bad: 0237\n",
      "Epoch: 024, Loss: 0.15963, Train Acc: 0.91741,Test Acc: 0.90625, TP: 194, FP: 29, TN: 154, FN: 07, Pred_good: 0161, Pred_bad: 0223\n",
      "Epoch: 025, Loss: 0.14538, Train Acc: 0.92857,Test Acc: 0.88802, TP: 168, FP: 10, TN: 173, FN: 33, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 026, Loss: 0.13880, Train Acc: 0.92708,Test Acc: 0.89323, TP: 179, FP: 19, TN: 164, FN: 22, Pred_good: 0186, Pred_bad: 0198\n",
      "Epoch: 027, Loss: 0.12835, Train Acc: 0.92783,Test Acc: 0.88281, TP: 167, FP: 11, TN: 172, FN: 34, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 028, Loss: 0.13140, Train Acc: 0.92932,Test Acc: 0.88021, TP: 162, FP: 07, TN: 176, FN: 39, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 029, Loss: 0.16058, Train Acc: 0.91369,Test Acc: 0.89583, TP: 183, FP: 22, TN: 161, FN: 18, Pred_good: 0179, Pred_bad: 0205\n",
      "Epoch: 030, Loss: 0.18481, Train Acc: 0.91667,Test Acc: 0.91927, TP: 201, FP: 31, TN: 152, FN: 00, Pred_good: 0152, Pred_bad: 0232\n",
      "Epoch: 031, Loss: 0.14155, Train Acc: 0.92336,Test Acc: 0.90625, TP: 190, FP: 25, TN: 158, FN: 11, Pred_good: 0169, Pred_bad: 0215\n",
      "Epoch: 032, Loss: 0.15015, Train Acc: 0.92708,Test Acc: 0.89583, TP: 174, FP: 13, TN: 170, FN: 27, Pred_good: 0197, Pred_bad: 0187\n",
      "Epoch: 033, Loss: 0.13894, Train Acc: 0.93080,Test Acc: 0.88281, TP: 175, FP: 19, TN: 164, FN: 26, Pred_good: 0190, Pred_bad: 0194\n",
      "Epoch: 034, Loss: 0.12833, Train Acc: 0.93452,Test Acc: 0.88802, TP: 165, FP: 07, TN: 176, FN: 36, Pred_good: 0212, Pred_bad: 0172\n",
      "Epoch: 035, Loss: 0.12052, Train Acc: 0.92634,Test Acc: 0.88021, TP: 162, FP: 07, TN: 176, FN: 39, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 036, Loss: 0.13994, Train Acc: 0.91518,Test Acc: 0.89062, TP: 187, FP: 28, TN: 155, FN: 14, Pred_good: 0169, Pred_bad: 0215\n",
      "Epoch: 037, Loss: 0.14128, Train Acc: 0.92485,Test Acc: 0.90365, TP: 185, FP: 21, TN: 162, FN: 16, Pred_good: 0178, Pred_bad: 0206\n",
      "Epoch: 038, Loss: 0.12173, Train Acc: 0.93229,Test Acc: 0.88802, TP: 165, FP: 07, TN: 176, FN: 36, Pred_good: 0212, Pred_bad: 0172\n",
      "Epoch: 039, Loss: 0.15143, Train Acc: 0.91741,Test Acc: 0.91927, TP: 201, FP: 31, TN: 152, FN: 00, Pred_good: 0152, Pred_bad: 0232\n",
      "Epoch: 040, Loss: 0.13071, Train Acc: 0.92932,Test Acc: 0.88021, TP: 162, FP: 07, TN: 176, FN: 39, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 041, Loss: 0.12083, Train Acc: 0.93006,Test Acc: 0.89062, TP: 170, FP: 11, TN: 172, FN: 31, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 042, Loss: 0.19899, Train Acc: 0.91890,Test Acc: 0.91406, TP: 191, FP: 23, TN: 160, FN: 10, Pred_good: 0170, Pred_bad: 0214\n",
      "Epoch: 043, Loss: 0.13209, Train Acc: 0.93676,Test Acc: 0.88542, TP: 170, FP: 13, TN: 170, FN: 31, Pred_good: 0201, Pred_bad: 0183\n",
      "Epoch: 044, Loss: 0.11867, Train Acc: 0.92559,Test Acc: 0.88802, TP: 164, FP: 06, TN: 177, FN: 37, Pred_good: 0214, Pred_bad: 0170\n",
      "Epoch: 045, Loss: 0.12027, Train Acc: 0.92708,Test Acc: 0.89323, TP: 176, FP: 16, TN: 167, FN: 25, Pred_good: 0192, Pred_bad: 0192\n",
      "Epoch: 046, Loss: 0.11738, Train Acc: 0.93229,Test Acc: 0.89062, TP: 170, FP: 11, TN: 172, FN: 31, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 047, Loss: 0.13042, Train Acc: 0.92262,Test Acc: 0.88802, TP: 158, FP: 00, TN: 183, FN: 43, Pred_good: 0226, Pred_bad: 0158\n",
      "Epoch: 048, Loss: 0.14756, Train Acc: 0.91518,Test Acc: 0.89062, TP: 163, FP: 04, TN: 179, FN: 38, Pred_good: 0217, Pred_bad: 0167\n",
      "Epoch: 049, Loss: 0.14836, Train Acc: 0.92485,Test Acc: 0.88281, TP: 167, FP: 11, TN: 172, FN: 34, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 050, Loss: 0.13281, Train Acc: 0.92857,Test Acc: 0.88021, TP: 162, FP: 07, TN: 176, FN: 39, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 051, Loss: 0.11809, Train Acc: 0.92634,Test Acc: 0.90625, TP: 184, FP: 19, TN: 164, FN: 17, Pred_good: 0181, Pred_bad: 0203\n",
      "Epoch: 052, Loss: 0.11945, Train Acc: 0.92559,Test Acc: 0.88281, TP: 167, FP: 11, TN: 172, FN: 34, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 053, Loss: 0.18524, Train Acc: 0.91518,Test Acc: 0.87760, TP: 188, FP: 34, TN: 149, FN: 13, Pred_good: 0162, Pred_bad: 0222\n",
      "Epoch: 054, Loss: 0.25115, Train Acc: 0.91592,Test Acc: 0.91406, TP: 199, FP: 31, TN: 152, FN: 02, Pred_good: 0154, Pred_bad: 0230\n",
      "Epoch: 055, Loss: 0.17262, Train Acc: 0.92485,Test Acc: 0.89062, TP: 184, FP: 25, TN: 158, FN: 17, Pred_good: 0175, Pred_bad: 0209\n",
      "Epoch: 056, Loss: 0.17015, Train Acc: 0.92039,Test Acc: 0.88802, TP: 175, FP: 17, TN: 166, FN: 26, Pred_good: 0192, Pred_bad: 0192\n",
      "Epoch: 057, Loss: 0.14723, Train Acc: 0.92783,Test Acc: 0.88281, TP: 163, FP: 07, TN: 176, FN: 38, Pred_good: 0214, Pred_bad: 0170\n",
      "Epoch: 058, Loss: 0.13040, Train Acc: 0.92857,Test Acc: 0.88281, TP: 163, FP: 07, TN: 176, FN: 38, Pred_good: 0214, Pred_bad: 0170\n",
      "Epoch: 059, Loss: 0.13019, Train Acc: 0.93080,Test Acc: 0.88542, TP: 178, FP: 21, TN: 162, FN: 23, Pred_good: 0185, Pred_bad: 0199\n",
      "Epoch: 060, Loss: 0.13782, Train Acc: 0.92485,Test Acc: 0.88021, TP: 178, FP: 23, TN: 160, FN: 23, Pred_good: 0183, Pred_bad: 0201\n",
      "Epoch: 061, Loss: 0.13921, Train Acc: 0.92336,Test Acc: 0.88021, TP: 167, FP: 12, TN: 171, FN: 34, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 062, Loss: 0.14050, Train Acc: 0.92559,Test Acc: 0.87760, TP: 169, FP: 15, TN: 168, FN: 32, Pred_good: 0200, Pred_bad: 0184\n",
      "Epoch: 063, Loss: 0.12976, Train Acc: 0.93155,Test Acc: 0.90104, TP: 188, FP: 25, TN: 158, FN: 13, Pred_good: 0171, Pred_bad: 0213\n",
      "Epoch: 064, Loss: 0.12477, Train Acc: 0.92113,Test Acc: 0.89062, TP: 175, FP: 16, TN: 167, FN: 26, Pred_good: 0193, Pred_bad: 0191\n",
      "Epoch: 065, Loss: 0.14641, Train Acc: 0.91592,Test Acc: 0.88281, TP: 160, FP: 04, TN: 179, FN: 41, Pred_good: 0220, Pred_bad: 0164\n",
      "Epoch: 066, Loss: 0.15468, Train Acc: 0.93229,Test Acc: 0.88281, TP: 178, FP: 22, TN: 161, FN: 23, Pred_good: 0184, Pred_bad: 0200\n",
      "Epoch: 067, Loss: 0.14049, Train Acc: 0.92932,Test Acc: 0.88021, TP: 162, FP: 07, TN: 176, FN: 39, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 068, Loss: 0.12133, Train Acc: 0.92336,Test Acc: 0.88021, TP: 167, FP: 12, TN: 171, FN: 34, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 069, Loss: 0.16569, Train Acc: 0.92411,Test Acc: 0.89583, TP: 178, FP: 17, TN: 166, FN: 23, Pred_good: 0189, Pred_bad: 0195\n",
      "Epoch: 070, Loss: 0.13995, Train Acc: 0.91741,Test Acc: 0.90625, TP: 194, FP: 29, TN: 154, FN: 07, Pred_good: 0161, Pred_bad: 0223\n",
      "Epoch: 071, Loss: 0.13653, Train Acc: 0.93006,Test Acc: 0.88021, TP: 162, FP: 07, TN: 176, FN: 39, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 072, Loss: 0.12567, Train Acc: 0.91741,Test Acc: 0.88281, TP: 179, FP: 23, TN: 160, FN: 22, Pred_good: 0182, Pred_bad: 0202\n",
      "Epoch: 073, Loss: 0.12350, Train Acc: 0.91443,Test Acc: 0.89844, TP: 181, FP: 19, TN: 164, FN: 20, Pred_good: 0184, Pred_bad: 0200\n",
      "Epoch: 074, Loss: 0.14144, Train Acc: 0.90848,Test Acc: 0.86458, TP: 167, FP: 18, TN: 165, FN: 34, Pred_good: 0199, Pred_bad: 0185\n",
      "Epoch: 075, Loss: 0.12601, Train Acc: 0.92336,Test Acc: 0.89062, TP: 163, FP: 04, TN: 179, FN: 38, Pred_good: 0217, Pred_bad: 0167\n",
      "Epoch: 076, Loss: 0.12992, Train Acc: 0.92783,Test Acc: 0.91406, TP: 180, FP: 12, TN: 171, FN: 21, Pred_good: 0192, Pred_bad: 0192\n",
      "Epoch: 077, Loss: 0.11830, Train Acc: 0.91443,Test Acc: 0.88802, TP: 183, FP: 25, TN: 158, FN: 18, Pred_good: 0176, Pred_bad: 0208\n",
      "Epoch: 078, Loss: 0.11709, Train Acc: 0.92485,Test Acc: 0.90365, TP: 182, FP: 18, TN: 165, FN: 19, Pred_good: 0184, Pred_bad: 0200\n",
      "Epoch: 079, Loss: 0.11536, Train Acc: 0.92336,Test Acc: 0.90104, TP: 182, FP: 19, TN: 164, FN: 19, Pred_good: 0183, Pred_bad: 0201\n",
      "Epoch: 080, Loss: 0.14790, Train Acc: 0.92708,Test Acc: 0.88281, TP: 163, FP: 07, TN: 176, FN: 38, Pred_good: 0214, Pred_bad: 0170\n",
      "Epoch: 081, Loss: 0.13535, Train Acc: 0.91964,Test Acc: 0.87760, TP: 165, FP: 11, TN: 172, FN: 36, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 082, Loss: 0.12207, Train Acc: 0.91816,Test Acc: 0.89844, TP: 179, FP: 17, TN: 166, FN: 22, Pred_good: 0188, Pred_bad: 0196\n",
      "Epoch: 083, Loss: 0.11827, Train Acc: 0.93080,Test Acc: 0.89583, TP: 161, FP: 00, TN: 183, FN: 40, Pred_good: 0223, Pred_bad: 0161\n",
      "Epoch: 084, Loss: 0.13773, Train Acc: 0.92559,Test Acc: 0.89062, TP: 184, FP: 25, TN: 158, FN: 17, Pred_good: 0175, Pred_bad: 0209\n",
      "Epoch: 085, Loss: 0.11381, Train Acc: 0.92708,Test Acc: 0.89062, TP: 167, FP: 08, TN: 175, FN: 34, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 086, Loss: 0.11349, Train Acc: 0.92485,Test Acc: 0.87240, TP: 159, FP: 07, TN: 176, FN: 42, Pred_good: 0218, Pred_bad: 0166\n",
      "Epoch: 087, Loss: 0.12771, Train Acc: 0.92188,Test Acc: 0.89062, TP: 163, FP: 04, TN: 179, FN: 38, Pred_good: 0217, Pred_bad: 0167\n",
      "Epoch: 088, Loss: 0.11056, Train Acc: 0.93080,Test Acc: 0.89323, TP: 160, FP: 00, TN: 183, FN: 41, Pred_good: 0224, Pred_bad: 0160\n",
      "Epoch: 089, Loss: 0.12346, Train Acc: 0.92857,Test Acc: 0.89062, TP: 169, FP: 10, TN: 173, FN: 32, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 090, Loss: 0.15111, Train Acc: 0.91741,Test Acc: 0.91927, TP: 201, FP: 31, TN: 152, FN: 00, Pred_good: 0152, Pred_bad: 0232\n",
      "Epoch: 091, Loss: 0.13657, Train Acc: 0.92411,Test Acc: 0.88281, TP: 167, FP: 11, TN: 172, FN: 34, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 092, Loss: 0.13406, Train Acc: 0.92188,Test Acc: 0.89583, TP: 186, FP: 25, TN: 158, FN: 15, Pred_good: 0173, Pred_bad: 0211\n",
      "Epoch: 093, Loss: 0.16608, Train Acc: 0.92559,Test Acc: 0.89844, TP: 191, FP: 29, TN: 154, FN: 10, Pred_good: 0164, Pred_bad: 0220\n",
      "Epoch: 094, Loss: 0.12529, Train Acc: 0.92634,Test Acc: 0.88281, TP: 175, FP: 19, TN: 164, FN: 26, Pred_good: 0190, Pred_bad: 0194\n",
      "Epoch: 095, Loss: 0.12634, Train Acc: 0.93006,Test Acc: 0.88542, TP: 176, FP: 19, TN: 164, FN: 25, Pred_good: 0189, Pred_bad: 0195\n",
      "Epoch: 096, Loss: 0.11884, Train Acc: 0.92188,Test Acc: 0.89583, TP: 178, FP: 17, TN: 166, FN: 23, Pred_good: 0189, Pred_bad: 0195\n",
      "Epoch: 097, Loss: 0.12092, Train Acc: 0.92932,Test Acc: 0.89583, TP: 161, FP: 00, TN: 183, FN: 40, Pred_good: 0223, Pred_bad: 0161\n",
      "Epoch: 098, Loss: 0.12050, Train Acc: 0.92485,Test Acc: 0.88021, TP: 176, FP: 21, TN: 162, FN: 25, Pred_good: 0187, Pred_bad: 0197\n",
      "Epoch: 099, Loss: 0.13770, Train Acc: 0.92336,Test Acc: 0.87760, TP: 167, FP: 13, TN: 170, FN: 34, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 100, Loss: 0.12455, Train Acc: 0.92783,Test Acc: 0.88021, TP: 178, FP: 23, TN: 160, FN: 23, Pred_good: 0183, Pred_bad: 0201\n",
      "Epoch: 101, Loss: 0.13215, Train Acc: 0.91146,Test Acc: 0.89583, TP: 190, FP: 29, TN: 154, FN: 11, Pred_good: 0165, Pred_bad: 0219\n",
      "Epoch: 102, Loss: 0.12507, Train Acc: 0.93006,Test Acc: 0.88021, TP: 162, FP: 07, TN: 176, FN: 39, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 103, Loss: 0.11949, Train Acc: 0.91890,Test Acc: 0.89323, TP: 170, FP: 10, TN: 173, FN: 31, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 104, Loss: 0.12190, Train Acc: 0.92411,Test Acc: 0.88542, TP: 169, FP: 12, TN: 171, FN: 32, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 105, Loss: 0.12323, Train Acc: 0.92411,Test Acc: 0.89583, TP: 178, FP: 17, TN: 166, FN: 23, Pred_good: 0189, Pred_bad: 0195\n",
      "Epoch: 106, Loss: 0.13075, Train Acc: 0.93006,Test Acc: 0.88021, TP: 162, FP: 07, TN: 176, FN: 39, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 107, Loss: 0.13571, Train Acc: 0.91369,Test Acc: 0.88281, TP: 160, FP: 04, TN: 179, FN: 41, Pred_good: 0220, Pred_bad: 0164\n",
      "Epoch: 108, Loss: 0.14920, Train Acc: 0.92262,Test Acc: 0.89844, TP: 164, FP: 02, TN: 181, FN: 37, Pred_good: 0218, Pred_bad: 0166\n",
      "Epoch: 109, Loss: 0.12016, Train Acc: 0.92039,Test Acc: 0.90104, TP: 175, FP: 12, TN: 171, FN: 26, Pred_good: 0197, Pred_bad: 0187\n",
      "Epoch: 110, Loss: 0.13322, Train Acc: 0.92932,Test Acc: 0.89062, TP: 167, FP: 08, TN: 175, FN: 34, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 111, Loss: 0.11067, Train Acc: 0.92634,Test Acc: 0.89844, TP: 187, FP: 25, TN: 158, FN: 14, Pred_good: 0172, Pred_bad: 0212\n",
      "Epoch: 112, Loss: 0.11136, Train Acc: 0.92708,Test Acc: 0.90885, TP: 182, FP: 16, TN: 167, FN: 19, Pred_good: 0186, Pred_bad: 0198\n",
      "Epoch: 113, Loss: 0.11249, Train Acc: 0.92857,Test Acc: 0.89583, TP: 182, FP: 21, TN: 162, FN: 19, Pred_good: 0181, Pred_bad: 0203\n",
      "Epoch: 114, Loss: 0.10893, Train Acc: 0.92634,Test Acc: 0.89323, TP: 172, FP: 12, TN: 171, FN: 29, Pred_good: 0200, Pred_bad: 0184\n",
      "Epoch: 115, Loss: 0.10962, Train Acc: 0.91964,Test Acc: 0.90365, TP: 180, FP: 16, TN: 167, FN: 21, Pred_good: 0188, Pred_bad: 0196\n",
      "Epoch: 116, Loss: 0.11022, Train Acc: 0.93304,Test Acc: 0.89583, TP: 171, FP: 10, TN: 173, FN: 30, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 117, Loss: 0.10798, Train Acc: 0.92336,Test Acc: 0.92188, TP: 196, FP: 25, TN: 158, FN: 05, Pred_good: 0163, Pred_bad: 0221\n",
      "Epoch: 118, Loss: 0.11460, Train Acc: 0.93006,Test Acc: 0.88021, TP: 162, FP: 07, TN: 176, FN: 39, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 119, Loss: 0.13790, Train Acc: 0.93378,Test Acc: 0.89323, TP: 168, FP: 08, TN: 175, FN: 33, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 120, Loss: 0.13655, Train Acc: 0.87872,Test Acc: 0.84115, TP: 140, FP: 00, TN: 183, FN: 61, Pred_good: 0244, Pred_bad: 0140\n",
      "Epoch: 121, Loss: 0.13461, Train Acc: 0.93229,Test Acc: 0.89583, TP: 174, FP: 13, TN: 170, FN: 27, Pred_good: 0197, Pred_bad: 0187\n",
      "Epoch: 122, Loss: 0.11800, Train Acc: 0.92262,Test Acc: 0.89844, TP: 187, FP: 25, TN: 158, FN: 14, Pred_good: 0172, Pred_bad: 0212\n",
      "Epoch: 123, Loss: 0.11717, Train Acc: 0.92708,Test Acc: 0.88281, TP: 167, FP: 11, TN: 172, FN: 34, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 124, Loss: 0.11400, Train Acc: 0.92708,Test Acc: 0.89844, TP: 164, FP: 02, TN: 181, FN: 37, Pred_good: 0218, Pred_bad: 0166\n",
      "Epoch: 125, Loss: 0.12253, Train Acc: 0.91443,Test Acc: 0.90104, TP: 172, FP: 09, TN: 174, FN: 29, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 126, Loss: 0.12455, Train Acc: 0.93080,Test Acc: 0.90365, TP: 179, FP: 15, TN: 168, FN: 22, Pred_good: 0190, Pred_bad: 0194\n",
      "Epoch: 127, Loss: 0.11105, Train Acc: 0.92932,Test Acc: 0.89062, TP: 167, FP: 08, TN: 175, FN: 34, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 128, Loss: 0.11164, Train Acc: 0.93676,Test Acc: 0.88802, TP: 175, FP: 17, TN: 166, FN: 26, Pred_good: 0192, Pred_bad: 0192\n",
      "Epoch: 129, Loss: 0.10858, Train Acc: 0.92634,Test Acc: 0.88802, TP: 158, FP: 00, TN: 183, FN: 43, Pred_good: 0226, Pred_bad: 0158\n",
      "Epoch: 130, Loss: 0.10839, Train Acc: 0.93080,Test Acc: 0.89844, TP: 164, FP: 02, TN: 181, FN: 37, Pred_good: 0218, Pred_bad: 0166\n",
      "Epoch: 131, Loss: 0.10921, Train Acc: 0.92039,Test Acc: 0.88802, TP: 158, FP: 00, TN: 183, FN: 43, Pred_good: 0226, Pred_bad: 0158\n",
      "Epoch: 132, Loss: 0.11498, Train Acc: 0.92559,Test Acc: 0.89062, TP: 167, FP: 08, TN: 175, FN: 34, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 133, Loss: 0.11948, Train Acc: 0.92634,Test Acc: 0.90365, TP: 181, FP: 17, TN: 166, FN: 20, Pred_good: 0186, Pred_bad: 0198\n",
      "Epoch: 134, Loss: 0.11069, Train Acc: 0.93080,Test Acc: 0.89844, TP: 164, FP: 02, TN: 181, FN: 37, Pred_good: 0218, Pred_bad: 0166\n",
      "Epoch: 135, Loss: 0.10885, Train Acc: 0.92857,Test Acc: 0.88542, TP: 165, FP: 08, TN: 175, FN: 36, Pred_good: 0211, Pred_bad: 0173\n",
      "Epoch: 136, Loss: 0.10877, Train Acc: 0.93155,Test Acc: 0.89844, TP: 164, FP: 02, TN: 181, FN: 37, Pred_good: 0218, Pred_bad: 0166\n",
      "Epoch: 137, Loss: 0.10857, Train Acc: 0.92783,Test Acc: 0.89583, TP: 183, FP: 22, TN: 161, FN: 18, Pred_good: 0179, Pred_bad: 0205\n",
      "Epoch: 138, Loss: 0.11004, Train Acc: 0.93080,Test Acc: 0.89323, TP: 174, FP: 14, TN: 169, FN: 27, Pred_good: 0196, Pred_bad: 0188\n",
      "Epoch: 139, Loss: 0.14139, Train Acc: 0.93006,Test Acc: 0.88802, TP: 166, FP: 08, TN: 175, FN: 35, Pred_good: 0210, Pred_bad: 0174\n",
      "Epoch: 140, Loss: 0.12949, Train Acc: 0.92485,Test Acc: 0.88542, TP: 157, FP: 00, TN: 183, FN: 44, Pred_good: 0227, Pred_bad: 0157\n",
      "Epoch: 141, Loss: 0.12769, Train Acc: 0.92634,Test Acc: 0.90625, TP: 184, FP: 19, TN: 164, FN: 17, Pred_good: 0181, Pred_bad: 0203\n",
      "Epoch: 142, Loss: 0.11424, Train Acc: 0.93304,Test Acc: 0.88802, TP: 179, FP: 21, TN: 162, FN: 22, Pred_good: 0184, Pred_bad: 0200\n",
      "Epoch: 143, Loss: 0.11832, Train Acc: 0.91816,Test Acc: 0.91406, TP: 186, FP: 18, TN: 165, FN: 15, Pred_good: 0180, Pred_bad: 0204\n",
      "Epoch: 144, Loss: 0.12119, Train Acc: 0.93229,Test Acc: 0.89844, TP: 170, FP: 08, TN: 175, FN: 31, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 145, Loss: 0.13674, Train Acc: 0.92113,Test Acc: 0.90885, TP: 185, FP: 19, TN: 164, FN: 16, Pred_good: 0180, Pred_bad: 0204\n",
      "Epoch: 146, Loss: 0.12178, Train Acc: 0.93080,Test Acc: 0.89844, TP: 176, FP: 14, TN: 169, FN: 25, Pred_good: 0194, Pred_bad: 0190\n",
      "Epoch: 147, Loss: 0.11884, Train Acc: 0.92857,Test Acc: 0.89062, TP: 167, FP: 08, TN: 175, FN: 34, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 148, Loss: 0.12363, Train Acc: 0.93006,Test Acc: 0.88021, TP: 162, FP: 07, TN: 176, FN: 39, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 149, Loss: 0.12295, Train Acc: 0.92262,Test Acc: 0.91406, TP: 183, FP: 15, TN: 168, FN: 18, Pred_good: 0186, Pred_bad: 0198\n",
      "Epoch: 150, Loss: 0.11758, Train Acc: 0.91741,Test Acc: 0.89844, TP: 169, FP: 07, TN: 176, FN: 32, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 151, Loss: 0.11207, Train Acc: 0.91592,Test Acc: 0.91406, TP: 179, FP: 11, TN: 172, FN: 22, Pred_good: 0194, Pred_bad: 0190\n",
      "Epoch: 152, Loss: 0.11341, Train Acc: 0.92559,Test Acc: 0.90625, TP: 181, FP: 16, TN: 167, FN: 20, Pred_good: 0187, Pred_bad: 0197\n",
      "Epoch: 153, Loss: 0.11702, Train Acc: 0.92188,Test Acc: 0.90104, TP: 166, FP: 03, TN: 180, FN: 35, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 154, Loss: 0.11996, Train Acc: 0.92336,Test Acc: 0.91406, TP: 184, FP: 16, TN: 167, FN: 17, Pred_good: 0184, Pred_bad: 0200\n",
      "Epoch: 155, Loss: 0.11995, Train Acc: 0.92262,Test Acc: 0.88802, TP: 158, FP: 00, TN: 183, FN: 43, Pred_good: 0226, Pred_bad: 0158\n",
      "Epoch: 156, Loss: 0.11869, Train Acc: 0.92188,Test Acc: 0.88281, TP: 160, FP: 04, TN: 179, FN: 41, Pred_good: 0220, Pred_bad: 0164\n",
      "Epoch: 157, Loss: 0.12755, Train Acc: 0.92932,Test Acc: 0.88542, TP: 164, FP: 07, TN: 176, FN: 37, Pred_good: 0213, Pred_bad: 0171\n",
      "Epoch: 158, Loss: 0.11411, Train Acc: 0.92857,Test Acc: 0.90104, TP: 175, FP: 12, TN: 171, FN: 26, Pred_good: 0197, Pred_bad: 0187\n",
      "Epoch: 159, Loss: 0.11830, Train Acc: 0.92411,Test Acc: 0.89323, TP: 172, FP: 12, TN: 171, FN: 29, Pred_good: 0200, Pred_bad: 0184\n",
      "Epoch: 160, Loss: 0.10817, Train Acc: 0.93304,Test Acc: 0.89062, TP: 169, FP: 10, TN: 173, FN: 32, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 161, Loss: 0.12233, Train Acc: 0.92336,Test Acc: 0.90365, TP: 178, FP: 14, TN: 169, FN: 23, Pred_good: 0192, Pred_bad: 0192\n",
      "Epoch: 162, Loss: 0.13303, Train Acc: 0.92634,Test Acc: 0.88542, TP: 159, FP: 02, TN: 181, FN: 42, Pred_good: 0223, Pred_bad: 0161\n",
      "Epoch: 163, Loss: 0.11993, Train Acc: 0.93155,Test Acc: 0.89583, TP: 171, FP: 10, TN: 173, FN: 30, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 164, Loss: 0.11422, Train Acc: 0.92708,Test Acc: 0.89062, TP: 167, FP: 08, TN: 175, FN: 34, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 165, Loss: 0.11320, Train Acc: 0.93229,Test Acc: 0.89583, TP: 166, FP: 05, TN: 178, FN: 35, Pred_good: 0213, Pred_bad: 0171\n",
      "Epoch: 166, Loss: 0.11890, Train Acc: 0.92783,Test Acc: 0.89062, TP: 167, FP: 08, TN: 175, FN: 34, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 167, Loss: 0.11189, Train Acc: 0.92708,Test Acc: 0.89323, TP: 172, FP: 12, TN: 171, FN: 29, Pred_good: 0200, Pred_bad: 0184\n",
      "Epoch: 168, Loss: 0.13484, Train Acc: 0.91592,Test Acc: 0.87760, TP: 165, FP: 11, TN: 172, FN: 36, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 169, Loss: 0.15195, Train Acc: 0.91741,Test Acc: 0.89323, TP: 177, FP: 17, TN: 166, FN: 24, Pred_good: 0190, Pred_bad: 0194\n",
      "Epoch: 170, Loss: 0.12778, Train Acc: 0.93155,Test Acc: 0.90104, TP: 172, FP: 09, TN: 174, FN: 29, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 171, Loss: 0.11758, Train Acc: 0.92857,Test Acc: 0.89062, TP: 172, FP: 13, TN: 170, FN: 29, Pred_good: 0199, Pred_bad: 0185\n",
      "Epoch: 172, Loss: 0.11840, Train Acc: 0.92634,Test Acc: 0.88021, TP: 166, FP: 11, TN: 172, FN: 35, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 173, Loss: 0.11578, Train Acc: 0.92857,Test Acc: 0.89323, TP: 167, FP: 07, TN: 176, FN: 34, Pred_good: 0210, Pred_bad: 0174\n",
      "Epoch: 174, Loss: 0.11228, Train Acc: 0.93080,Test Acc: 0.89844, TP: 164, FP: 02, TN: 181, FN: 37, Pred_good: 0218, Pred_bad: 0166\n",
      "Epoch: 175, Loss: 0.11427, Train Acc: 0.92559,Test Acc: 0.88542, TP: 167, FP: 10, TN: 173, FN: 34, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 176, Loss: 0.11521, Train Acc: 0.92857,Test Acc: 0.89062, TP: 167, FP: 08, TN: 175, FN: 34, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 177, Loss: 0.11316, Train Acc: 0.92559,Test Acc: 0.89323, TP: 172, FP: 12, TN: 171, FN: 29, Pred_good: 0200, Pred_bad: 0184\n",
      "Epoch: 178, Loss: 0.11183, Train Acc: 0.93452,Test Acc: 0.89062, TP: 168, FP: 09, TN: 174, FN: 33, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 179, Loss: 0.12074, Train Acc: 0.93452,Test Acc: 0.88802, TP: 165, FP: 07, TN: 176, FN: 36, Pred_good: 0212, Pred_bad: 0172\n",
      "Epoch: 180, Loss: 0.10948, Train Acc: 0.93006,Test Acc: 0.89323, TP: 173, FP: 13, TN: 170, FN: 28, Pred_good: 0198, Pred_bad: 0186\n",
      "Epoch: 181, Loss: 0.11521, Train Acc: 0.93080,Test Acc: 0.89844, TP: 164, FP: 02, TN: 181, FN: 37, Pred_good: 0218, Pred_bad: 0166\n",
      "Epoch: 182, Loss: 0.10877, Train Acc: 0.92857,Test Acc: 0.90104, TP: 169, FP: 06, TN: 177, FN: 32, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 183, Loss: 0.10851, Train Acc: 0.92857,Test Acc: 0.89062, TP: 167, FP: 08, TN: 175, FN: 34, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 184, Loss: 0.11049, Train Acc: 0.92634,Test Acc: 0.88802, TP: 158, FP: 00, TN: 183, FN: 43, Pred_good: 0226, Pred_bad: 0158\n",
      "Epoch: 185, Loss: 0.10789, Train Acc: 0.92857,Test Acc: 0.89062, TP: 167, FP: 08, TN: 175, FN: 34, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 186, Loss: 0.10725, Train Acc: 0.93080,Test Acc: 0.89844, TP: 164, FP: 02, TN: 181, FN: 37, Pred_good: 0218, Pred_bad: 0166\n",
      "Epoch: 187, Loss: 0.10770, Train Acc: 0.92411,Test Acc: 0.89323, TP: 165, FP: 05, TN: 178, FN: 36, Pred_good: 0214, Pred_bad: 0170\n",
      "Epoch: 188, Loss: 0.10689, Train Acc: 0.92932,Test Acc: 0.89062, TP: 167, FP: 08, TN: 175, FN: 34, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 189, Loss: 0.11219, Train Acc: 0.92485,Test Acc: 0.90885, TP: 177, FP: 11, TN: 172, FN: 24, Pred_good: 0196, Pred_bad: 0188\n",
      "Epoch: 190, Loss: 0.11043, Train Acc: 0.92708,Test Acc: 0.89062, TP: 159, FP: 00, TN: 183, FN: 42, Pred_good: 0225, Pred_bad: 0159\n",
      "Epoch: 191, Loss: 0.10772, Train Acc: 0.92485,Test Acc: 0.90104, TP: 172, FP: 09, TN: 174, FN: 29, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 192, Loss: 0.12840, Train Acc: 0.90476,Test Acc: 0.90104, TP: 193, FP: 30, TN: 153, FN: 08, Pred_good: 0161, Pred_bad: 0223\n",
      "Epoch: 193, Loss: 0.15190, Train Acc: 0.92932,Test Acc: 0.89583, TP: 175, FP: 14, TN: 169, FN: 26, Pred_good: 0195, Pred_bad: 0189\n",
      "Epoch: 194, Loss: 0.12974, Train Acc: 0.92485,Test Acc: 0.90885, TP: 193, FP: 27, TN: 156, FN: 08, Pred_good: 0164, Pred_bad: 0220\n",
      "Epoch: 195, Loss: 0.11556, Train Acc: 0.92634,Test Acc: 0.89323, TP: 172, FP: 12, TN: 171, FN: 29, Pred_good: 0200, Pred_bad: 0184\n",
      "Epoch: 196, Loss: 0.11799, Train Acc: 0.92857,Test Acc: 0.89062, TP: 167, FP: 08, TN: 175, FN: 34, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 197, Loss: 0.11377, Train Acc: 0.92783,Test Acc: 0.88281, TP: 167, FP: 11, TN: 172, FN: 34, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 198, Loss: 0.11752, Train Acc: 0.92708,Test Acc: 0.89062, TP: 167, FP: 08, TN: 175, FN: 34, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 199, Loss: 0.12227, Train Acc: 0.93452,Test Acc: 0.89062, TP: 166, FP: 07, TN: 176, FN: 35, Pred_good: 0211, Pred_bad: 0173\n",
      "train & test time span :  809.2153601646423\n"
     ]
    }
   ],
   "source": [
    "# train & test\n",
    "# time log\n",
    "time_start = time.time()\n",
    "\n",
    "train_loss_a = np.zeros(200)\n",
    "test_acc_a = np.zeros(200)\n",
    "for epoch in range(0, 200):\n",
    "    loss = train(epoch)\n",
    "    train_acc, tp, fp, tn, fn, pred_good, pred_bad = test(train_loader)\n",
    "    test_acc, tp, fp, tn, fn, pred_good, pred_bad = test(test_loader)\n",
    "    train_loss_a[epoch] = loss\n",
    "    test_acc_a[epoch] = test_acc\n",
    "    print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f},Test Acc: {:.5f}, TP: {:02d}, FP: {:02d}, TN: {:02d}, FN: {:02d}, Pred_good: {:04d}, Pred_bad: {:04d}'.\n",
    "          format(epoch, loss, train_acc, test_acc, tp, fp, tn, fn, pred_good, pred_bad))\n",
    "\n",
    "# fin train & test;;print time span\n",
    "time_fin_train = time.time()\n",
    "print(\"train & test time span : \", time_fin_train - time_start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

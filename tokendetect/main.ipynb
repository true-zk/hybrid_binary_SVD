{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gendata import gendata\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "CWE = 401"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data time span: 12.066455841064453\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max of code text length: 77\n"
     ]
    }
   ],
   "source": [
    "# start load data\n",
    "time_start = time.time()\n",
    "\n",
    "CWE = 'CWE{}'.format(CWE)\n",
    "path0 = r'D:\\Desktop\\hybrid-SVD\\datasrc\\{}\\badres\\dsm_extract'.format(CWE)\n",
    "path1 = r'D:\\Desktop\\hybrid-SVD\\datasrc\\{}\\goodres\\dsm_extract'.format(CWE)\n",
    "\n",
    "dataset = gendata(path0, CWE)\n",
    "dataset += gendata(path1, CWE)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "len_dataset = len(dataset)\n",
    "\n",
    "time_fin = time.time()\n",
    "print(\"load data time span:\", time_fin - time_start)\n",
    "\n",
    "data_len = []\n",
    "for data in dataset:\n",
    "    data_len.append(len(data['x']))\n",
    "plt.hist(data_len)\n",
    "plt.show()\n",
    "print(\"max of code text length:\", max(data_len))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "size = [75, 45]  # instruction vec size = 45. 175 from the histogram\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset_, size_ = size):\n",
    "        for idx, data_ in enumerate(dataset_):\n",
    "            data_len_ = len(data_['x'])\n",
    "            if data_len_ < size_[0]:\n",
    "                pad = torch.zeros(size_[0] - data_len_, size_[1])\n",
    "                dataset_[idx]['x'] = torch.cat((data_['x'], pad), dim=0)\n",
    "            else:\n",
    "                dataset_[idx]['x'] = data_['x'][:size_[0]]\n",
    "        self.dateset = dataset_\n",
    "        self.size = size_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dateset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dateset[idx]['x'], self.dateset[idx]['y']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of dataset:1396,         len of train:977,         len of validate:140,         len of test:279\n"
     ]
    }
   ],
   "source": [
    "# train\\test\\validate set\n",
    "\n",
    "len_train = int(0.7 * len_dataset)\n",
    "len_test = int(0.2 * len_dataset)\n",
    "len_validate = len_dataset - len_train - len_test\n",
    "\n",
    "Train_set = MyDataset(dataset[:len_train])\n",
    "Test_set = MyDataset(dataset[len_train: len_train + len_test])\n",
    "Validate_set = MyDataset(dataset[len_train + len_test:])\n",
    "\n",
    "train_loader = DataLoader(Train_set, batch_size=32, shuffle=False, drop_last=True )\n",
    "test_loader = DataLoader(Test_set, batch_size=32, shuffle=False, drop_last=True)\n",
    "validate_loader = DataLoader(Validate_set, batch_size=32, shuffle=False, drop_last=True)\n",
    "\n",
    "print(f\"len of dataset:{len_dataset}, \\\n",
    "        len of train:{len_train}, \\\n",
    "        len of validate:{len_validate}, \\\n",
    "        len of test:{len_test}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# TextCNN model\n",
    "# input: batch * h = 175 * w = 45\n",
    "# output: mlp with softmax\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, size_h, size_w):\n",
    "        super(TextCNN, self).__init__()\n",
    "        h_input = size_h\n",
    "        w_input = size_w\n",
    "        # Text_CNN layer\n",
    "        conv_sizes = [2, 4, 6, 8, 10, 12, 16, 20]\n",
    "        self.num_filter = len(conv_sizes)\n",
    "        num_conv_per_size = 64\n",
    "        self.cnv0 = nn.ModuleList([nn.Conv2d(1, num_conv_per_size, (cnv_size, 45)) for cnv_size in conv_sizes])\n",
    "        # Attention layer\n",
    "        self.max = nn.MaxPool2d(kernel_size=(num_conv_per_size, 1))\n",
    "        self.avg = nn.AvgPool2d(kernel_size=(num_conv_per_size, 1))\n",
    "        h_raw = [h_input - i + 1 for i in conv_sizes]  # 175 - conv_size +1\n",
    "        conv_sizes1 = [math.ceil(i/2) for i in h_raw]\n",
    "        self.cnv1 = nn.ModuleList([nn.Conv2d(2, 1, (cnv_size, 1), padding='same') for cnv_size in conv_sizes1])\n",
    "        self.sig = nn.Sigmoid()\n",
    "        # Output layer\n",
    "        h_res = sum(h_raw)  # input_feature\n",
    "        self.lin0 = nn.Linear(h_res, 512)\n",
    "        self.lin1 = nn.Linear(512, 128)\n",
    "        self.lin2 = nn.Linear(128, 64)\n",
    "        self.lin3 = nn.Linear(64, 2)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # text CNN\n",
    "        # x = batch * h * w\n",
    "        x = torch.unsqueeze(x, 1)  # batch, channel_in=1 , h, w\n",
    "        x = [torch.squeeze(F.relu(conv(x)), 3) for conv in self.cnv0]  # each_filter: batch, channel_out=64, h'\n",
    "        # x: num_filter * batch * num_per_filter=64 * h'\n",
    "\n",
    "        # attention\n",
    "        x = [torch.unsqueeze(i, 1) for i in x]  # num_filter * [ batch * 1 * num_per_filter=64 * h' ]\n",
    "\n",
    "        max_ = [self.max(i) for i in x]  # num_filter * [batch * 1 * 1 * h' ]\n",
    "        max_ = [torch.unsqueeze(torch.squeeze(i, 1), 3) for i in max_]  # num_filter * [batch * 1 * h' * 1]\n",
    "        avg_ = [self.avg(i) for i in x]  # num_filter * [batch * 1 * 1 * h' ]\n",
    "        avg_ = [torch.unsqueeze(torch.squeeze(i, 1), 3) for i in avg_]  # num_filter * [batch * 1 * h' * 1]\n",
    "\n",
    "        attention = [torch.cat((max_[i], avg_[i]), dim=1) for i in range(0, self.num_filter)]  # num_filter * [batch * 2 * h' * 1]\n",
    "        attention = [self.sig(self.cnv1[i](attention[i])) for i in range(0, self.num_filter)]  # num_filter * [batch * 1 * h' * 1]\n",
    "        attention = [torch.unsqueeze(torch.squeeze(i, 3), 1) for i in attention]  # num_filter * [batch * 1 * 1 * h']\n",
    "\n",
    "        x = [x[i] * attention[i] for i in range(0, self.num_filter)]  # num_filter * [ batch * 1 * num_per_filter=64 * h' ]\n",
    "        x = [torch.squeeze(self.max(i)) for i in x]  # num_filter * [batch * h']\n",
    "        x = torch.cat(x, dim=1)  # batch * Î£(h')\n",
    "        x = torch.squeeze(x)\n",
    "        # out\n",
    "        x = F.relu(self.lin0(x))\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TextCNN(size[0], size[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        labels = torch.squeeze(labels.view(1, -1))\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        y_pred = model(inputs)\n",
    "        loss = F.nll_loss(y_pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_all += loss.item() * len(inputs)\n",
    "    return loss_all / len(train_loader.dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    pred_good = 0\n",
    "    pred_bad = 0\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = torch.squeeze(labels.view(1, -1))\n",
    "        labels = labels.to(device)\n",
    "        y_pred = model(inputs)\n",
    "        y_pred = y_pred.max(dim=1)[1]\n",
    "        correct += y_pred.eq(labels).sum().item()\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 0 and labels[i] == 0:\n",
    "                tp += 1\n",
    "                pred_bad += 1\n",
    "            elif y_pred[i] == 0 and labels[i] == 1:\n",
    "                fp += 1\n",
    "                pred_bad += 1\n",
    "            elif y_pred[i] == 1 and labels[i] == 0:\n",
    "                fn += 1\n",
    "                pred_good += 1\n",
    "            elif y_pred[i] == 1 and labels[i] == 1:\n",
    "                tn += 1\n",
    "                pred_good += 1\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return round(acc, 6), tp, fp, tn, fn, pred_good, pred_bad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4563 4563\n",
      "1294 1294\n",
      "Epoch: 000, Loss: 0.14224, Test Acc: 0.95700, Test Acc: 0.96280\n",
      "Test: TP: 0617, FP: 0014, TN: 0677, FN: 0036, Pred_good: 0713, Pred_bad: 0631\n",
      "4591 4591\n",
      "1282 1282\n",
      "Epoch: 001, Loss: 0.08468, Test Acc: 0.96288, Test Acc: 0.95387\n",
      "Test: TP: 0642, FP: 0051, TN: 0640, FN: 0011, Pred_good: 0651, Pred_bad: 0693\n",
      "4603 4603\n",
      "1288 1288\n",
      "Epoch: 002, Loss: 0.07811, Test Acc: 0.96539, Test Acc: 0.95833\n",
      "Test: TP: 0653, FP: 0056, TN: 0635, FN: 0000, Pred_good: 0635, Pred_bad: 0709\n",
      "4606 4606\n",
      "1289 1289\n",
      "Epoch: 003, Loss: 0.06069, Test Acc: 0.96602, Test Acc: 0.95908\n",
      "Test: TP: 0653, FP: 0055, TN: 0636, FN: 0000, Pred_good: 0636, Pred_bad: 0708\n",
      "4619 4619\n",
      "1291 1291\n",
      "Epoch: 004, Loss: 0.05288, Test Acc: 0.96875, Test Acc: 0.96057\n",
      "Test: TP: 0652, FP: 0052, TN: 0639, FN: 0001, Pred_good: 0640, Pred_bad: 0704\n",
      "4609 4609\n",
      "1289 1289\n",
      "Epoch: 005, Loss: 0.05810, Test Acc: 0.96665, Test Acc: 0.95908\n",
      "Test: TP: 0652, FP: 0054, TN: 0637, FN: 0001, Pred_good: 0638, Pred_bad: 0706\n",
      "4631 4631\n",
      "1294 1294\n",
      "Epoch: 006, Loss: 0.06134, Test Acc: 0.97127, Test Acc: 0.96280\n",
      "Test: TP: 0653, FP: 0050, TN: 0641, FN: 0000, Pred_good: 0641, Pred_bad: 0703\n",
      "4631 4631\n",
      "1294 1294\n",
      "Epoch: 007, Loss: 0.05250, Test Acc: 0.97127, Test Acc: 0.96280\n",
      "Test: TP: 0653, FP: 0050, TN: 0641, FN: 0000, Pred_good: 0641, Pred_bad: 0703\n",
      "4606 4606\n",
      "1287 1287\n",
      "Epoch: 008, Loss: 0.05013, Test Acc: 0.96602, Test Acc: 0.95759\n",
      "Test: TP: 0648, FP: 0052, TN: 0639, FN: 0005, Pred_good: 0644, Pred_bad: 0700\n",
      "4642 4642\n",
      "1294 1294\n",
      "Epoch: 009, Loss: 0.05671, Test Acc: 0.97357, Test Acc: 0.96280\n",
      "Test: TP: 0653, FP: 0050, TN: 0641, FN: 0000, Pred_good: 0641, Pred_bad: 0703\n",
      "4631 4631\n",
      "1294 1294\n",
      "Epoch: 010, Loss: 0.04633, Test Acc: 0.97127, Test Acc: 0.96280\n",
      "Test: TP: 0653, FP: 0050, TN: 0641, FN: 0000, Pred_good: 0641, Pred_bad: 0703\n",
      "4644 4644\n",
      "1295 1295\n",
      "Epoch: 011, Loss: 0.04020, Test Acc: 0.97399, Test Acc: 0.96354\n",
      "Test: TP: 0653, FP: 0049, TN: 0642, FN: 0000, Pred_good: 0642, Pred_bad: 0702\n",
      "4651 4651\n",
      "1296 1296\n",
      "Epoch: 012, Loss: 0.03926, Test Acc: 0.97546, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4651 4651\n",
      "1296 1296\n",
      "Epoch: 013, Loss: 0.03724, Test Acc: 0.97546, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4651 4651\n",
      "1296 1296\n",
      "Epoch: 014, Loss: 0.03676, Test Acc: 0.97546, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4653 4653\n",
      "1294 1294\n",
      "Epoch: 015, Loss: 0.03668, Test Acc: 0.97588, Test Acc: 0.96280\n",
      "Test: TP: 0651, FP: 0048, TN: 0643, FN: 0002, Pred_good: 0645, Pred_bad: 0699\n",
      "4651 4651\n",
      "1296 1296\n",
      "Epoch: 016, Loss: 0.03666, Test Acc: 0.97546, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4651 4651\n",
      "1296 1296\n",
      "Epoch: 017, Loss: 0.03670, Test Acc: 0.97546, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4651 4651\n",
      "1296 1296\n",
      "Epoch: 018, Loss: 0.03662, Test Acc: 0.97546, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4651 4651\n",
      "1296 1296\n",
      "Epoch: 019, Loss: 0.03643, Test Acc: 0.97546, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4651 4651\n",
      "1296 1296\n",
      "Epoch: 020, Loss: 0.03655, Test Acc: 0.97546, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4651 4651\n",
      "1296 1296\n",
      "Epoch: 021, Loss: 0.03664, Test Acc: 0.97546, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4651 4651\n",
      "1296 1296\n",
      "Epoch: 022, Loss: 0.03641, Test Acc: 0.97546, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4611 4611\n",
      "1284 1284\n",
      "Epoch: 023, Loss: 0.05161, Test Acc: 0.96707, Test Acc: 0.95536\n",
      "Test: TP: 0645, FP: 0052, TN: 0639, FN: 0008, Pred_good: 0647, Pred_bad: 0697\n",
      "4623 4623\n",
      "1291 1291\n",
      "Epoch: 024, Loss: 0.07139, Test Acc: 0.96959, Test Acc: 0.96057\n",
      "Test: TP: 0653, FP: 0053, TN: 0638, FN: 0000, Pred_good: 0638, Pred_bad: 0706\n",
      "4654 4654\n",
      "1296 1296\n",
      "Epoch: 025, Loss: 0.03947, Test Acc: 0.97609, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4654 4654\n",
      "1296 1296\n",
      "Epoch: 026, Loss: 0.03570, Test Acc: 0.97609, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4656 4656\n",
      "1294 1294\n",
      "Epoch: 027, Loss: 0.04683, Test Acc: 0.97651, Test Acc: 0.96280\n",
      "Test: TP: 0646, FP: 0043, TN: 0648, FN: 0007, Pred_good: 0655, Pred_bad: 0689\n",
      "4654 4654\n",
      "1296 1296\n",
      "Epoch: 028, Loss: 0.03677, Test Acc: 0.97609, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4654 4654\n",
      "1296 1296\n",
      "Epoch: 029, Loss: 0.03579, Test Acc: 0.97609, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4654 4654\n",
      "1296 1296\n",
      "Epoch: 030, Loss: 0.03769, Test Acc: 0.97609, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4654 4654\n",
      "1296 1296\n",
      "Epoch: 031, Loss: 0.03574, Test Acc: 0.97609, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n",
      "4654 4654\n",
      "1296 1296\n",
      "Epoch: 032, Loss: 0.03589, Test Acc: 0.97609, Test Acc: 0.96429\n",
      "Test: TP: 0653, FP: 0048, TN: 0643, FN: 0000, Pred_good: 0643, Pred_bad: 0701\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m200\u001B[39m):\n\u001B[0;32m      7\u001B[0m     loss \u001B[38;5;241m=\u001B[39m train()\n\u001B[1;32m----> 8\u001B[0m     train_acc, tp_, fp_, tn_, fn_, pred_good_, pred_bad_ \u001B[38;5;241m=\u001B[39m \u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     test_acc, tp_, fp_, tn_, fn_, pred_good_, pred_bad_ \u001B[38;5;241m=\u001B[39m test(test_loader)\n\u001B[0;32m     10\u001B[0m     train_loss_a[epoch] \u001B[38;5;241m=\u001B[39m loss\n",
      "Cell \u001B[1;32mIn[27], line 15\u001B[0m, in \u001B[0;36mtest\u001B[1;34m(loader)\u001B[0m\n\u001B[0;32m     13\u001B[0m labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msqueeze(labels\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     14\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 15\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m y_pred\u001B[38;5;241m.\u001B[39mmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     17\u001B[0m correct \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m y_pred\u001B[38;5;241m.\u001B[39meq(labels)\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32mD:\\Develop\\Miniconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[22], line 34\u001B[0m, in \u001B[0;36mTextCNN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# text CNN\u001B[39;00m\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;66;03m# x = batch * h * w\u001B[39;00m\n\u001B[0;32m     33\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39munsqueeze(x, \u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# batch, channel_in=1 , h, w\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m     x \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39msqueeze(F\u001B[38;5;241m.\u001B[39mrelu(conv(x)), \u001B[38;5;241m3\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m conv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcnv0]  \u001B[38;5;66;03m# each_filter: batch, channel_out=64, h'\u001B[39;00m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;66;03m# x: num_filter * batch * num_per_filter=64 * h'\u001B[39;00m\n\u001B[0;32m     36\u001B[0m \n\u001B[0;32m     37\u001B[0m     \u001B[38;5;66;03m# attention\u001B[39;00m\n\u001B[0;32m     38\u001B[0m     x \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39munsqueeze(i, \u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m x]  \u001B[38;5;66;03m# num_filter * [ batch * 1 * num_per_filter=64 * h' ]\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[22], line 34\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# text CNN\u001B[39;00m\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;66;03m# x = batch * h * w\u001B[39;00m\n\u001B[0;32m     33\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39munsqueeze(x, \u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# batch, channel_in=1 , h, w\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m     x \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39msqueeze(\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;241m3\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m conv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcnv0]  \u001B[38;5;66;03m# each_filter: batch, channel_out=64, h'\u001B[39;00m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;66;03m# x: num_filter * batch * num_per_filter=64 * h'\u001B[39;00m\n\u001B[0;32m     36\u001B[0m \n\u001B[0;32m     37\u001B[0m     \u001B[38;5;66;03m# attention\u001B[39;00m\n\u001B[0;32m     38\u001B[0m     x \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39munsqueeze(i, \u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m x]  \u001B[38;5;66;03m# num_filter * [ batch * 1 * num_per_filter=64 * h' ]\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Develop\\Miniconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001B[0m, in \u001B[0;36mrelu\u001B[1;34m(input, inplace)\u001B[0m\n\u001B[0;32m   1455\u001B[0m     result \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu_(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m   1456\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1457\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1458\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# train & test\n",
    "time_start = time.time()\n",
    "\n",
    "train_loss_a = np.zeros(200)\n",
    "test_acc_a = np.zeros(200)\n",
    "for epoch in range(0,200):\n",
    "    if epoch % 10 == 0:\n",
    "        train_acc, tp_, fp_, tn_, fn_, pred_good_, pred_bad_ = test(train_loader)\n",
    "        test_acc, tp_, fp_, tn_, fn_, pred_good_, pred_bad_ = test(test_loader)\n",
    "        train_loss_a[epoch] = loss\n",
    "        test_acc_a[epoch] = test_acc\n",
    "        print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}, Test Acc: {:.5f}'.\n",
    "              format(epoch, loss, train_acc, test_acc, ))\n",
    "        print('Test: TP: {:04d}, FP: {:04d}, TN: {:04d}, FN: {:04d}, Pred_good: {:04d}, Pred_bad: {:04d}'.\n",
    "              format(tp_, fp_, tn_, fn_, pred_good_, pred_bad_))\n",
    "        loss = train()\n",
    "\n",
    "print(\"train & test time span:\", time.time() - time_start)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

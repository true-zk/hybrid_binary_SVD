{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gendata import gendata\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data time span: 39.359724044799805\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max of code text length: 212\n"
     ]
    }
   ],
   "source": [
    "# start load data\n",
    "time_start = time.time()\n",
    "\n",
    "CWE = 'CWE23'\n",
    "path0 = r'D:\\Desktop\\hybrid-SVD\\datasrc\\{}\\{}_bad\\dsm_extract\\\\'.format(CWE, CWE)\n",
    "path1 = r'D:\\Desktop\\hybrid-SVD\\datasrc\\{}\\{}_good\\dsm_extract\\\\'.format(CWE, CWE)\n",
    "\n",
    "dataset = gendata(path0, CWE)\n",
    "dataset += gendata(path1, CWE)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "len_dataset = len(dataset)\n",
    "\n",
    "time_fin = time.time()\n",
    "print(\"load data time span:\", time_fin - time_start)\n",
    "\n",
    "data_len = []\n",
    "for data in dataset:\n",
    "    data_len.append(len(data['x']))\n",
    "plt.hist(data_len)\n",
    "plt.show()\n",
    "print(\"max of code text length:\", max(data_len))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "size = [175, 45]  # instruction vec size = 45. 175 from the histogram\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset_, size_ = size):\n",
    "        for idx, data_ in enumerate(dataset_):\n",
    "            data_len_ = len(data_['x'])\n",
    "            if data_len_ < size_[0]:\n",
    "                pad = torch.zeros(size[0] - data_len_, size[1])\n",
    "                dataset_[idx]['x'] = torch.cat((data_['x'], pad), dim=0)\n",
    "            else:\n",
    "                dataset_[idx]['x'] = data_['x'][:size_[0]]\n",
    "        self.dateset = dataset_\n",
    "        self.size = size_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dateset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dateset[idx]['x'], self.dateset[idx]['y']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of dataset:1920,         len of train:1344,         len of validate:192,         len of test:384\n"
     ]
    }
   ],
   "source": [
    "# train\\test\\validate set\n",
    "\n",
    "len_train = int(0.7 * len_dataset)\n",
    "len_test = int(0.2 * len_dataset)\n",
    "len_validate = len_dataset - len_train - len_test\n",
    "\n",
    "Train_set = MyDataset(dataset[:len_train])\n",
    "Test_set = MyDataset(dataset[len_train: len_train + len_test])\n",
    "Validate_set = MyDataset(dataset[len_train + len_test:])\n",
    "\n",
    "train_loader = DataLoader(Train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(Test_set, batch_size=32, shuffle=True)\n",
    "validate_loader = DataLoader(Validate_set, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"len of dataset:{len_dataset}, \\\n",
    "        len of train:{len_train}, \\\n",
    "        len of validate:{len_validate}, \\\n",
    "        len of test:{len_test}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# TextCNN model\n",
    "# input: batch * h = 175 * w = 45\n",
    "# output: mlp with softmax\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "        h_input = 175\n",
    "        w_input = 45\n",
    "        # Text_CNN layer\n",
    "        conv_sizes = [2, 4, 6, 8, 10, 12, 16, 20]\n",
    "        self.num_filter = len(conv_sizes)\n",
    "        num_conv_per_size = 64\n",
    "        self.cnv0 = nn.ModuleList([nn.Conv2d(1, num_conv_per_size, (cnv_size, 45)) for cnv_size in conv_sizes])\n",
    "        # Attention layer\n",
    "        self.max = nn.MaxPool2d(kernel_size=(num_conv_per_size, 1))\n",
    "        self.avg = nn.AvgPool2d(kernel_size=(num_conv_per_size, 1))\n",
    "        h_raw = [h_input - i + 1 for i in conv_sizes]  # 175 - conv_size +1\n",
    "        conv_sizes1 = [math.ceil(i/2) for i in h_raw]\n",
    "        self.cnv1 = nn.ModuleList([nn.Conv2d(2, 1, (cnv_size, 1), padding='same') for cnv_size in conv_sizes1])\n",
    "        self.sig = nn.Sigmoid()\n",
    "        # Output layer\n",
    "        h_res = sum(h_raw)  # input_feature\n",
    "        self.lin0 = nn.Linear(h_res, 512)\n",
    "        self.lin1 = nn.Linear(512, 128)\n",
    "        self.lin2 = nn.Linear(128, 64)\n",
    "        self.lin3 = nn.Linear(64, 2)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # text CNN\n",
    "        # x = batch * h * w\n",
    "        x = torch.unsqueeze(x, 1)  # batch, channel_in=1 , h, w\n",
    "        x = [torch.squeeze(F.relu(conv(x)), 3) for conv in self.cnv0]  # each_filter: batch, channel_out=64, h'\n",
    "        # x: num_filter * batch * num_per_filter=64 * h'\n",
    "\n",
    "        # attention\n",
    "        x = [torch.unsqueeze(i, 1) for i in x]  # num_filter * [ batch * 1 * num_per_filter=64 * h' ]\n",
    "\n",
    "        max_ = [self.max(i) for i in x]  # num_filter * [batch * 1 * 1 * h' ]\n",
    "        max_ = [torch.unsqueeze(torch.squeeze(i, 1), 3) for i in max_]  # num_filter * [batch * 1 * h' * 1]\n",
    "        avg_ = [self.avg(i) for i in x]  # num_filter * [batch * 1 * 1 * h' ]\n",
    "        avg_ = [torch.unsqueeze(torch.squeeze(i, 1), 3) for i in avg_]  # num_filter * [batch * 1 * h' * 1]\n",
    "\n",
    "        attention = [torch.cat((max_[i], avg_[i]), dim=1) for i in range(0, self.num_filter)]  # num_filter * [batch * 2 * h' * 1]\n",
    "        attention = [self.sig(self.cnv1[i](attention[i])) for i in range(0, self.num_filter)]  # num_filter * [batch * 1 * h' * 1]\n",
    "        attention = [torch.unsqueeze(torch.squeeze(i, 3), 1) for i in attention]  # num_filter * [batch * 1 * 1 * h']\n",
    "\n",
    "        x = [x[i] * attention[i] for i in range(0, self.num_filter)]  # num_filter * [ batch * 1 * num_per_filter=64 * h' ]\n",
    "        x = [torch.squeeze(self.max(i)) for i in x]  # num_filter * [batch * h']\n",
    "        x = torch.cat(x, dim=1)  # batch * Î£(h')\n",
    "        x = torch.squeeze(x)\n",
    "        # out\n",
    "        x = F.relu(self.lin0(x))\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TextCNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        labels = torch.squeeze(labels.view(1, -1))\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        y_pred = model(inputs)\n",
    "        loss = F.nll_loss(y_pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_all += loss.item() * len(inputs)\n",
    "    return loss_all / len(train_loader.dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    pred_good = 0\n",
    "    pred_bad = 0\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = torch.squeeze(labels.view(1, -1))\n",
    "        labels = labels.to(device)\n",
    "        y_pred = model(inputs).max(dim=1)[1]\n",
    "        correct += y_pred.eq(labels).sum().item() ################################\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == 0 and labels[i] == 0:\n",
    "                tp += 1\n",
    "                pred_bad += 1\n",
    "            elif y_pred[i] == 0 and labels[i] == 1:\n",
    "                fp += 1\n",
    "                pred_bad += 1\n",
    "            elif y_pred[i] == 1 and labels[i] == 0:\n",
    "                fn += 1\n",
    "                pred_good += 1\n",
    "            elif y_pred[i] == 1 and labels[i] == 1:\n",
    "                tn += 1\n",
    "                pred_good += 1\n",
    "    acc = (correct / len(loader.dataset))\n",
    "    return round(acc, 6), tp, fp, tn, fn, pred_good, pred_bad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Develop\\Miniconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Convolution.cpp:883.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.46430,Test Acc: 0.90885\n",
      "Test: TP: 0156, FP: 0001, TN: 0193, FN: 0034, Pred_good: 0227, Pred_bad: 0157\n",
      "Epoch: 001, Loss: 0.15980,Test Acc: 0.92188\n",
      "Test: TP: 0166, FP: 0006, TN: 0188, FN: 0024, Pred_good: 0212, Pred_bad: 0172\n",
      "Epoch: 002, Loss: 0.13366,Test Acc: 0.92188\n",
      "Test: TP: 0183, FP: 0023, TN: 0171, FN: 0007, Pred_good: 0178, Pred_bad: 0206\n",
      "Epoch: 003, Loss: 0.12375,Test Acc: 0.94010\n",
      "Test: TP: 0168, FP: 0001, TN: 0193, FN: 0022, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 004, Loss: 0.12107,Test Acc: 0.93229\n",
      "Test: TP: 0166, FP: 0002, TN: 0192, FN: 0024, Pred_good: 0216, Pred_bad: 0168\n",
      "Epoch: 005, Loss: 0.12026,Test Acc: 0.93490\n",
      "Test: TP: 0180, FP: 0015, TN: 0179, FN: 0010, Pred_good: 0189, Pred_bad: 0195\n",
      "Epoch: 006, Loss: 0.11707,Test Acc: 0.93490\n",
      "Test: TP: 0190, FP: 0025, TN: 0169, FN: 0000, Pred_good: 0169, Pred_bad: 0215\n",
      "Epoch: 007, Loss: 0.11468,Test Acc: 0.91927\n",
      "Test: TP: 0161, FP: 0002, TN: 0192, FN: 0029, Pred_good: 0221, Pred_bad: 0163\n",
      "Epoch: 008, Loss: 0.11217,Test Acc: 0.93229\n",
      "Test: TP: 0171, FP: 0007, TN: 0187, FN: 0019, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 009, Loss: 0.12937,Test Acc: 0.93229\n",
      "Test: TP: 0190, FP: 0026, TN: 0168, FN: 0000, Pred_good: 0168, Pred_bad: 0216\n",
      "Epoch: 010, Loss: 0.12301,Test Acc: 0.91927\n",
      "Test: TP: 0161, FP: 0002, TN: 0192, FN: 0029, Pred_good: 0221, Pred_bad: 0163\n",
      "Epoch: 011, Loss: 0.11367,Test Acc: 0.92448\n",
      "Test: TP: 0161, FP: 0000, TN: 0194, FN: 0029, Pred_good: 0223, Pred_bad: 0161\n",
      "Epoch: 012, Loss: 0.11530,Test Acc: 0.91146\n",
      "Test: TP: 0161, FP: 0005, TN: 0189, FN: 0029, Pred_good: 0218, Pred_bad: 0166\n",
      "Epoch: 013, Loss: 0.10883,Test Acc: 0.92448\n",
      "Test: TP: 0185, FP: 0024, TN: 0170, FN: 0005, Pred_good: 0175, Pred_bad: 0209\n",
      "Epoch: 014, Loss: 0.11343,Test Acc: 0.93229\n",
      "Test: TP: 0188, FP: 0024, TN: 0170, FN: 0002, Pred_good: 0172, Pred_bad: 0212\n",
      "Epoch: 015, Loss: 0.10244,Test Acc: 0.93490\n",
      "Test: TP: 0167, FP: 0002, TN: 0192, FN: 0023, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 016, Loss: 0.09429,Test Acc: 0.92969\n",
      "Test: TP: 0179, FP: 0016, TN: 0178, FN: 0011, Pred_good: 0189, Pred_bad: 0195\n",
      "Epoch: 017, Loss: 0.10662,Test Acc: 0.94271\n",
      "Test: TP: 0172, FP: 0004, TN: 0190, FN: 0018, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 018, Loss: 0.08560,Test Acc: 0.92188\n",
      "Test: TP: 0165, FP: 0005, TN: 0189, FN: 0025, Pred_good: 0214, Pred_bad: 0170\n",
      "Epoch: 019, Loss: 0.09339,Test Acc: 0.94792\n",
      "Test: TP: 0170, FP: 0000, TN: 0194, FN: 0020, Pred_good: 0214, Pred_bad: 0170\n",
      "Epoch: 020, Loss: 0.10279,Test Acc: 0.94531\n",
      "Test: TP: 0181, FP: 0012, TN: 0182, FN: 0009, Pred_good: 0191, Pred_bad: 0193\n",
      "Epoch: 021, Loss: 0.10631,Test Acc: 0.92969\n",
      "Test: TP: 0169, FP: 0006, TN: 0188, FN: 0021, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 022, Loss: 0.07553,Test Acc: 0.94531\n",
      "Test: TP: 0169, FP: 0000, TN: 0194, FN: 0021, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 023, Loss: 0.07757,Test Acc: 0.93490\n",
      "Test: TP: 0167, FP: 0002, TN: 0192, FN: 0023, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 024, Loss: 0.06895,Test Acc: 0.93229\n",
      "Test: TP: 0172, FP: 0008, TN: 0186, FN: 0018, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 025, Loss: 0.12204,Test Acc: 0.91667\n",
      "Test: TP: 0170, FP: 0012, TN: 0182, FN: 0020, Pred_good: 0202, Pred_bad: 0182\n",
      "Epoch: 026, Loss: 0.11554,Test Acc: 0.91146\n",
      "Test: TP: 0161, FP: 0005, TN: 0189, FN: 0029, Pred_good: 0218, Pred_bad: 0166\n",
      "Epoch: 027, Loss: 0.09094,Test Acc: 0.92188\n",
      "Test: TP: 0165, FP: 0005, TN: 0189, FN: 0025, Pred_good: 0214, Pred_bad: 0170\n",
      "Epoch: 028, Loss: 0.07729,Test Acc: 0.94010\n",
      "Test: TP: 0183, FP: 0016, TN: 0178, FN: 0007, Pred_good: 0185, Pred_bad: 0199\n",
      "Epoch: 029, Loss: 0.08105,Test Acc: 0.94271\n",
      "Test: TP: 0178, FP: 0010, TN: 0184, FN: 0012, Pred_good: 0196, Pred_bad: 0188\n",
      "Epoch: 030, Loss: 0.09584,Test Acc: 0.92448\n",
      "Test: TP: 0166, FP: 0005, TN: 0189, FN: 0024, Pred_good: 0213, Pred_bad: 0171\n",
      "Epoch: 031, Loss: 0.07275,Test Acc: 0.92969\n",
      "Test: TP: 0163, FP: 0000, TN: 0194, FN: 0027, Pred_good: 0221, Pred_bad: 0163\n",
      "Epoch: 032, Loss: 0.07127,Test Acc: 0.96094\n",
      "Test: TP: 0183, FP: 0008, TN: 0186, FN: 0007, Pred_good: 0193, Pred_bad: 0191\n",
      "Epoch: 033, Loss: 0.06668,Test Acc: 0.96094\n",
      "Test: TP: 0176, FP: 0001, TN: 0193, FN: 0014, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 034, Loss: 0.06691,Test Acc: 0.93750\n",
      "Test: TP: 0173, FP: 0007, TN: 0187, FN: 0017, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 035, Loss: 0.06286,Test Acc: 0.95312\n",
      "Test: TP: 0179, FP: 0007, TN: 0187, FN: 0011, Pred_good: 0198, Pred_bad: 0186\n",
      "Epoch: 036, Loss: 0.05951,Test Acc: 0.94531\n",
      "Test: TP: 0174, FP: 0005, TN: 0189, FN: 0016, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 037, Loss: 0.07253,Test Acc: 0.93490\n",
      "Test: TP: 0165, FP: 0000, TN: 0194, FN: 0025, Pred_good: 0219, Pred_bad: 0165\n",
      "Epoch: 038, Loss: 0.07802,Test Acc: 0.93229\n",
      "Test: TP: 0171, FP: 0007, TN: 0187, FN: 0019, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 039, Loss: 0.07104,Test Acc: 0.95312\n",
      "Test: TP: 0172, FP: 0000, TN: 0194, FN: 0018, Pred_good: 0212, Pred_bad: 0172\n",
      "Epoch: 040, Loss: 0.06059,Test Acc: 0.95052\n",
      "Test: TP: 0173, FP: 0002, TN: 0192, FN: 0017, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 041, Loss: 0.06140,Test Acc: 0.93490\n",
      "Test: TP: 0172, FP: 0007, TN: 0187, FN: 0018, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 042, Loss: 0.09438,Test Acc: 0.93490\n",
      "Test: TP: 0165, FP: 0000, TN: 0194, FN: 0025, Pred_good: 0219, Pred_bad: 0165\n",
      "Epoch: 043, Loss: 0.06760,Test Acc: 0.95573\n",
      "Test: TP: 0175, FP: 0002, TN: 0192, FN: 0015, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 044, Loss: 0.06822,Test Acc: 0.95312\n",
      "Test: TP: 0180, FP: 0008, TN: 0186, FN: 0010, Pred_good: 0196, Pred_bad: 0188\n",
      "Epoch: 045, Loss: 0.06629,Test Acc: 0.94010\n",
      "Test: TP: 0169, FP: 0002, TN: 0192, FN: 0021, Pred_good: 0213, Pred_bad: 0171\n",
      "Epoch: 046, Loss: 0.06627,Test Acc: 0.95573\n",
      "Test: TP: 0173, FP: 0000, TN: 0194, FN: 0017, Pred_good: 0211, Pred_bad: 0173\n",
      "Epoch: 047, Loss: 0.06292,Test Acc: 0.94010\n",
      "Test: TP: 0172, FP: 0005, TN: 0189, FN: 0018, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 048, Loss: 0.05697,Test Acc: 0.95052\n",
      "Test: TP: 0184, FP: 0013, TN: 0181, FN: 0006, Pred_good: 0187, Pred_bad: 0197\n",
      "Epoch: 049, Loss: 0.05840,Test Acc: 0.95052\n",
      "Test: TP: 0184, FP: 0013, TN: 0181, FN: 0006, Pred_good: 0187, Pred_bad: 0197\n",
      "Epoch: 050, Loss: 0.05991,Test Acc: 0.94531\n",
      "Test: TP: 0177, FP: 0008, TN: 0186, FN: 0013, Pred_good: 0199, Pred_bad: 0185\n",
      "Epoch: 051, Loss: 0.05573,Test Acc: 0.95052\n",
      "Test: TP: 0175, FP: 0004, TN: 0190, FN: 0015, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 052, Loss: 0.05785,Test Acc: 0.93490\n",
      "Test: TP: 0178, FP: 0013, TN: 0181, FN: 0012, Pred_good: 0193, Pred_bad: 0191\n",
      "Epoch: 053, Loss: 0.05552,Test Acc: 0.94792\n",
      "Test: TP: 0170, FP: 0000, TN: 0194, FN: 0020, Pred_good: 0214, Pred_bad: 0170\n",
      "Epoch: 054, Loss: 0.05837,Test Acc: 0.94792\n",
      "Test: TP: 0172, FP: 0002, TN: 0192, FN: 0018, Pred_good: 0210, Pred_bad: 0174\n",
      "Epoch: 055, Loss: 0.05310,Test Acc: 0.92708\n",
      "Test: TP: 0164, FP: 0002, TN: 0192, FN: 0026, Pred_good: 0218, Pred_bad: 0166\n",
      "Epoch: 056, Loss: 0.08336,Test Acc: 0.92708\n",
      "Test: TP: 0169, FP: 0007, TN: 0187, FN: 0021, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 057, Loss: 0.06550,Test Acc: 0.95312\n",
      "Test: TP: 0172, FP: 0000, TN: 0194, FN: 0018, Pred_good: 0212, Pred_bad: 0172\n",
      "Epoch: 058, Loss: 0.05039,Test Acc: 0.94531\n",
      "Test: TP: 0174, FP: 0005, TN: 0189, FN: 0016, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 059, Loss: 0.06067,Test Acc: 0.94531\n",
      "Test: TP: 0171, FP: 0002, TN: 0192, FN: 0019, Pred_good: 0211, Pred_bad: 0173\n",
      "Epoch: 060, Loss: 0.05073,Test Acc: 0.94792\n",
      "Test: TP: 0172, FP: 0002, TN: 0192, FN: 0018, Pred_good: 0210, Pred_bad: 0174\n",
      "Epoch: 061, Loss: 0.05757,Test Acc: 0.94010\n",
      "Test: TP: 0177, FP: 0010, TN: 0184, FN: 0013, Pred_good: 0197, Pred_bad: 0187\n",
      "Epoch: 062, Loss: 0.09823,Test Acc: 0.93490\n",
      "Test: TP: 0165, FP: 0000, TN: 0194, FN: 0025, Pred_good: 0219, Pred_bad: 0165\n",
      "Epoch: 063, Loss: 0.06244,Test Acc: 0.96615\n",
      "Test: TP: 0177, FP: 0000, TN: 0194, FN: 0013, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 064, Loss: 0.06247,Test Acc: 0.94792\n",
      "Test: TP: 0176, FP: 0006, TN: 0188, FN: 0014, Pred_good: 0202, Pred_bad: 0182\n",
      "Epoch: 065, Loss: 0.04939,Test Acc: 0.94792\n",
      "Test: TP: 0184, FP: 0014, TN: 0180, FN: 0006, Pred_good: 0186, Pred_bad: 0198\n",
      "Epoch: 066, Loss: 0.05640,Test Acc: 0.94010\n",
      "Test: TP: 0174, FP: 0007, TN: 0187, FN: 0016, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 067, Loss: 0.05163,Test Acc: 0.96354\n",
      "Test: TP: 0178, FP: 0002, TN: 0192, FN: 0012, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 068, Loss: 0.05307,Test Acc: 0.95312\n",
      "Test: TP: 0172, FP: 0000, TN: 0194, FN: 0018, Pred_good: 0212, Pred_bad: 0172\n",
      "Epoch: 069, Loss: 0.04468,Test Acc: 0.95312\n",
      "Test: TP: 0172, FP: 0000, TN: 0194, FN: 0018, Pred_good: 0212, Pred_bad: 0172\n",
      "Epoch: 070, Loss: 0.05375,Test Acc: 0.94531\n",
      "Test: TP: 0173, FP: 0004, TN: 0190, FN: 0017, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 071, Loss: 0.04329,Test Acc: 0.95833\n",
      "Test: TP: 0174, FP: 0000, TN: 0194, FN: 0016, Pred_good: 0210, Pred_bad: 0174\n",
      "Epoch: 072, Loss: 0.03875,Test Acc: 0.94792\n",
      "Test: TP: 0175, FP: 0005, TN: 0189, FN: 0015, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 073, Loss: 0.04832,Test Acc: 0.95312\n",
      "Test: TP: 0174, FP: 0002, TN: 0192, FN: 0016, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 074, Loss: 0.04284,Test Acc: 0.95573\n",
      "Test: TP: 0175, FP: 0002, TN: 0192, FN: 0015, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 075, Loss: 0.04003,Test Acc: 0.95312\n",
      "Test: TP: 0177, FP: 0005, TN: 0189, FN: 0013, Pred_good: 0202, Pred_bad: 0182\n",
      "Epoch: 076, Loss: 0.07255,Test Acc: 0.93229\n",
      "Test: TP: 0166, FP: 0002, TN: 0192, FN: 0024, Pred_good: 0216, Pred_bad: 0168\n",
      "Epoch: 077, Loss: 0.05165,Test Acc: 0.94531\n",
      "Test: TP: 0175, FP: 0006, TN: 0188, FN: 0015, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 078, Loss: 0.04205,Test Acc: 0.96094\n",
      "Test: TP: 0177, FP: 0002, TN: 0192, FN: 0013, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 079, Loss: 0.04245,Test Acc: 0.96094\n",
      "Test: TP: 0175, FP: 0000, TN: 0194, FN: 0015, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 080, Loss: 0.04681,Test Acc: 0.95573\n",
      "Test: TP: 0175, FP: 0002, TN: 0192, FN: 0015, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 081, Loss: 0.05193,Test Acc: 0.94792\n",
      "Test: TP: 0179, FP: 0009, TN: 0185, FN: 0011, Pred_good: 0196, Pred_bad: 0188\n",
      "Epoch: 082, Loss: 0.04865,Test Acc: 0.95312\n",
      "Test: TP: 0174, FP: 0002, TN: 0192, FN: 0016, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 083, Loss: 0.04456,Test Acc: 0.95052\n",
      "Test: TP: 0173, FP: 0002, TN: 0192, FN: 0017, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 084, Loss: 0.03800,Test Acc: 0.95573\n",
      "Test: TP: 0175, FP: 0002, TN: 0192, FN: 0015, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 085, Loss: 0.04879,Test Acc: 0.97396\n",
      "Test: TP: 0183, FP: 0003, TN: 0191, FN: 0007, Pred_good: 0198, Pred_bad: 0186\n",
      "Epoch: 086, Loss: 0.05641,Test Acc: 0.95312\n",
      "Test: TP: 0172, FP: 0000, TN: 0194, FN: 0018, Pred_good: 0212, Pred_bad: 0172\n",
      "Epoch: 087, Loss: 0.03878,Test Acc: 0.95833\n",
      "Test: TP: 0176, FP: 0002, TN: 0192, FN: 0014, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 088, Loss: 0.03231,Test Acc: 0.95312\n",
      "Test: TP: 0180, FP: 0008, TN: 0186, FN: 0010, Pred_good: 0196, Pred_bad: 0188\n",
      "Epoch: 089, Loss: 0.04253,Test Acc: 0.95052\n",
      "Test: TP: 0173, FP: 0002, TN: 0192, FN: 0017, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 090, Loss: 0.02807,Test Acc: 0.96615\n",
      "Test: TP: 0180, FP: 0003, TN: 0191, FN: 0010, Pred_good: 0201, Pred_bad: 0183\n",
      "Epoch: 091, Loss: 0.02994,Test Acc: 0.97396\n",
      "Test: TP: 0183, FP: 0003, TN: 0191, FN: 0007, Pred_good: 0198, Pred_bad: 0186\n",
      "Epoch: 092, Loss: 0.03162,Test Acc: 0.97656\n",
      "Test: TP: 0183, FP: 0002, TN: 0192, FN: 0007, Pred_good: 0199, Pred_bad: 0185\n",
      "Epoch: 093, Loss: 0.02752,Test Acc: 0.95052\n",
      "Test: TP: 0173, FP: 0002, TN: 0192, FN: 0017, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 094, Loss: 0.04441,Test Acc: 0.96354\n",
      "Test: TP: 0178, FP: 0002, TN: 0192, FN: 0012, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 095, Loss: 0.03650,Test Acc: 0.95573\n",
      "Test: TP: 0175, FP: 0002, TN: 0192, FN: 0015, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 096, Loss: 0.02919,Test Acc: 0.95833\n",
      "Test: TP: 0176, FP: 0002, TN: 0192, FN: 0014, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 097, Loss: 0.03140,Test Acc: 0.96094\n",
      "Test: TP: 0175, FP: 0000, TN: 0194, FN: 0015, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 098, Loss: 0.03475,Test Acc: 0.96615\n",
      "Test: TP: 0177, FP: 0000, TN: 0194, FN: 0013, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 099, Loss: 0.03812,Test Acc: 0.96094\n",
      "Test: TP: 0177, FP: 0002, TN: 0192, FN: 0013, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 100, Loss: 0.03184,Test Acc: 0.96875\n",
      "Test: TP: 0178, FP: 0000, TN: 0194, FN: 0012, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 101, Loss: 0.03029,Test Acc: 0.96354\n",
      "Test: TP: 0178, FP: 0002, TN: 0192, FN: 0012, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 102, Loss: 0.02196,Test Acc: 0.95833\n",
      "Test: TP: 0176, FP: 0002, TN: 0192, FN: 0014, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 103, Loss: 0.01783,Test Acc: 0.96354\n",
      "Test: TP: 0176, FP: 0000, TN: 0194, FN: 0014, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 104, Loss: 0.02495,Test Acc: 0.96094\n",
      "Test: TP: 0175, FP: 0000, TN: 0194, FN: 0015, Pred_good: 0209, Pred_bad: 0175\n",
      "Epoch: 105, Loss: 0.04885,Test Acc: 0.96615\n",
      "Test: TP: 0178, FP: 0001, TN: 0193, FN: 0012, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 106, Loss: 0.04649,Test Acc: 0.94531\n",
      "Test: TP: 0169, FP: 0000, TN: 0194, FN: 0021, Pred_good: 0215, Pred_bad: 0169\n",
      "Epoch: 107, Loss: 0.07434,Test Acc: 0.95312\n",
      "Test: TP: 0174, FP: 0002, TN: 0192, FN: 0016, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 108, Loss: 0.04395,Test Acc: 0.95833\n",
      "Test: TP: 0175, FP: 0001, TN: 0193, FN: 0015, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 109, Loss: 0.03219,Test Acc: 0.97917\n",
      "Test: TP: 0182, FP: 0000, TN: 0194, FN: 0008, Pred_good: 0202, Pred_bad: 0182\n",
      "Epoch: 110, Loss: 0.03370,Test Acc: 0.96615\n",
      "Test: TP: 0177, FP: 0000, TN: 0194, FN: 0013, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 111, Loss: 0.03377,Test Acc: 0.96615\n",
      "Test: TP: 0179, FP: 0002, TN: 0192, FN: 0011, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 112, Loss: 0.02214,Test Acc: 0.96615\n",
      "Test: TP: 0177, FP: 0000, TN: 0194, FN: 0013, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 113, Loss: 0.02391,Test Acc: 0.96354\n",
      "Test: TP: 0179, FP: 0003, TN: 0191, FN: 0011, Pred_good: 0202, Pred_bad: 0182\n",
      "Epoch: 114, Loss: 0.02366,Test Acc: 0.97396\n",
      "Test: TP: 0180, FP: 0000, TN: 0194, FN: 0010, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 115, Loss: 0.03054,Test Acc: 0.96354\n",
      "Test: TP: 0178, FP: 0002, TN: 0192, FN: 0012, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 116, Loss: 0.01960,Test Acc: 0.96354\n",
      "Test: TP: 0176, FP: 0000, TN: 0194, FN: 0014, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 117, Loss: 0.01963,Test Acc: 0.96615\n",
      "Test: TP: 0177, FP: 0000, TN: 0194, FN: 0013, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 118, Loss: 0.03124,Test Acc: 0.96615\n",
      "Test: TP: 0177, FP: 0000, TN: 0194, FN: 0013, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 119, Loss: 0.09518,Test Acc: 0.94792\n",
      "Test: TP: 0172, FP: 0002, TN: 0192, FN: 0018, Pred_good: 0210, Pred_bad: 0174\n",
      "Epoch: 120, Loss: 0.03669,Test Acc: 0.96615\n",
      "Test: TP: 0177, FP: 0000, TN: 0194, FN: 0013, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 121, Loss: 0.03642,Test Acc: 0.96094\n",
      "Test: TP: 0177, FP: 0002, TN: 0192, FN: 0013, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 122, Loss: 0.03341,Test Acc: 0.96354\n",
      "Test: TP: 0176, FP: 0000, TN: 0194, FN: 0014, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 123, Loss: 0.02022,Test Acc: 0.95833\n",
      "Test: TP: 0174, FP: 0000, TN: 0194, FN: 0016, Pred_good: 0210, Pred_bad: 0174\n",
      "Epoch: 124, Loss: 0.04130,Test Acc: 0.97135\n",
      "Test: TP: 0182, FP: 0003, TN: 0191, FN: 0008, Pred_good: 0199, Pred_bad: 0185\n",
      "Epoch: 125, Loss: 0.04211,Test Acc: 0.94792\n",
      "Test: TP: 0170, FP: 0000, TN: 0194, FN: 0020, Pred_good: 0214, Pred_bad: 0170\n",
      "Epoch: 126, Loss: 0.04194,Test Acc: 0.95833\n",
      "Test: TP: 0176, FP: 0002, TN: 0192, FN: 0014, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 127, Loss: 0.02279,Test Acc: 0.97917\n",
      "Test: TP: 0182, FP: 0000, TN: 0194, FN: 0008, Pred_good: 0202, Pred_bad: 0182\n",
      "Epoch: 128, Loss: 0.01764,Test Acc: 0.97396\n",
      "Test: TP: 0183, FP: 0003, TN: 0191, FN: 0007, Pred_good: 0198, Pred_bad: 0186\n",
      "Epoch: 129, Loss: 0.01828,Test Acc: 0.97396\n",
      "Test: TP: 0182, FP: 0002, TN: 0192, FN: 0008, Pred_good: 0200, Pred_bad: 0184\n",
      "Epoch: 130, Loss: 0.01924,Test Acc: 0.96875\n",
      "Test: TP: 0178, FP: 0000, TN: 0194, FN: 0012, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 131, Loss: 0.01381,Test Acc: 0.97917\n",
      "Test: TP: 0182, FP: 0000, TN: 0194, FN: 0008, Pred_good: 0202, Pred_bad: 0182\n",
      "Epoch: 132, Loss: 0.01804,Test Acc: 0.97135\n",
      "Test: TP: 0183, FP: 0004, TN: 0190, FN: 0007, Pred_good: 0197, Pred_bad: 0187\n",
      "Epoch: 133, Loss: 0.02003,Test Acc: 0.97396\n",
      "Test: TP: 0184, FP: 0004, TN: 0190, FN: 0006, Pred_good: 0196, Pred_bad: 0188\n",
      "Epoch: 134, Loss: 0.02552,Test Acc: 0.97656\n",
      "Test: TP: 0181, FP: 0000, TN: 0194, FN: 0009, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 135, Loss: 0.01258,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 136, Loss: 0.01034,Test Acc: 0.98177\n",
      "Test: TP: 0183, FP: 0000, TN: 0194, FN: 0007, Pred_good: 0201, Pred_bad: 0183\n",
      "Epoch: 137, Loss: 0.01213,Test Acc: 0.95833\n",
      "Test: TP: 0174, FP: 0000, TN: 0194, FN: 0016, Pred_good: 0210, Pred_bad: 0174\n",
      "Epoch: 138, Loss: 0.04300,Test Acc: 0.97917\n",
      "Test: TP: 0182, FP: 0000, TN: 0194, FN: 0008, Pred_good: 0202, Pred_bad: 0182\n",
      "Epoch: 139, Loss: 0.01706,Test Acc: 0.97396\n",
      "Test: TP: 0180, FP: 0000, TN: 0194, FN: 0010, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 140, Loss: 0.01368,Test Acc: 0.97656\n",
      "Test: TP: 0181, FP: 0000, TN: 0194, FN: 0009, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 141, Loss: 0.01589,Test Acc: 0.97917\n",
      "Test: TP: 0182, FP: 0000, TN: 0194, FN: 0008, Pred_good: 0202, Pred_bad: 0182\n",
      "Epoch: 142, Loss: 0.01641,Test Acc: 0.96615\n",
      "Test: TP: 0179, FP: 0002, TN: 0192, FN: 0011, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 143, Loss: 0.01062,Test Acc: 0.97396\n",
      "Test: TP: 0180, FP: 0000, TN: 0194, FN: 0010, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 144, Loss: 0.01023,Test Acc: 0.97396\n",
      "Test: TP: 0180, FP: 0000, TN: 0194, FN: 0010, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 145, Loss: 0.02948,Test Acc: 0.97917\n",
      "Test: TP: 0184, FP: 0002, TN: 0192, FN: 0006, Pred_good: 0198, Pred_bad: 0186\n",
      "Epoch: 146, Loss: 0.04799,Test Acc: 0.96875\n",
      "Test: TP: 0178, FP: 0000, TN: 0194, FN: 0012, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 147, Loss: 0.01898,Test Acc: 0.97396\n",
      "Test: TP: 0180, FP: 0000, TN: 0194, FN: 0010, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 148, Loss: 0.02173,Test Acc: 0.96875\n",
      "Test: TP: 0178, FP: 0000, TN: 0194, FN: 0012, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 149, Loss: 0.03561,Test Acc: 0.96875\n",
      "Test: TP: 0178, FP: 0000, TN: 0194, FN: 0012, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 150, Loss: 0.01693,Test Acc: 0.97396\n",
      "Test: TP: 0180, FP: 0000, TN: 0194, FN: 0010, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 151, Loss: 0.01579,Test Acc: 0.97396\n",
      "Test: TP: 0180, FP: 0000, TN: 0194, FN: 0010, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 152, Loss: 0.00831,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 153, Loss: 0.01660,Test Acc: 0.97396\n",
      "Test: TP: 0180, FP: 0000, TN: 0194, FN: 0010, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 154, Loss: 0.02098,Test Acc: 0.96354\n",
      "Test: TP: 0176, FP: 0000, TN: 0194, FN: 0014, Pred_good: 0208, Pred_bad: 0176\n",
      "Epoch: 155, Loss: 0.02068,Test Acc: 0.98177\n",
      "Test: TP: 0183, FP: 0000, TN: 0194, FN: 0007, Pred_good: 0201, Pred_bad: 0183\n",
      "Epoch: 156, Loss: 0.01246,Test Acc: 0.97135\n",
      "Test: TP: 0180, FP: 0001, TN: 0193, FN: 0010, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 157, Loss: 0.01044,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 158, Loss: 0.01258,Test Acc: 0.97917\n",
      "Test: TP: 0183, FP: 0001, TN: 0193, FN: 0007, Pred_good: 0200, Pred_bad: 0184\n",
      "Epoch: 159, Loss: 0.00744,Test Acc: 0.96875\n",
      "Test: TP: 0180, FP: 0002, TN: 0192, FN: 0010, Pred_good: 0202, Pred_bad: 0182\n",
      "Epoch: 160, Loss: 0.00990,Test Acc: 0.98177\n",
      "Test: TP: 0183, FP: 0000, TN: 0194, FN: 0007, Pred_good: 0201, Pred_bad: 0183\n",
      "Epoch: 161, Loss: 0.01290,Test Acc: 0.98177\n",
      "Test: TP: 0183, FP: 0000, TN: 0194, FN: 0007, Pred_good: 0201, Pred_bad: 0183\n",
      "Epoch: 162, Loss: 0.02199,Test Acc: 0.98177\n",
      "Test: TP: 0183, FP: 0000, TN: 0194, FN: 0007, Pred_good: 0201, Pred_bad: 0183\n",
      "Epoch: 163, Loss: 0.01524,Test Acc: 0.97396\n",
      "Test: TP: 0180, FP: 0000, TN: 0194, FN: 0010, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 164, Loss: 0.00812,Test Acc: 0.97135\n",
      "Test: TP: 0183, FP: 0004, TN: 0190, FN: 0007, Pred_good: 0197, Pred_bad: 0187\n",
      "Epoch: 165, Loss: 0.02368,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 166, Loss: 0.07114,Test Acc: 0.97396\n",
      "Test: TP: 0180, FP: 0000, TN: 0194, FN: 0010, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 167, Loss: 0.02389,Test Acc: 0.96615\n",
      "Test: TP: 0179, FP: 0002, TN: 0192, FN: 0011, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 168, Loss: 0.02205,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 169, Loss: 0.01085,Test Acc: 0.97396\n",
      "Test: TP: 0180, FP: 0000, TN: 0194, FN: 0010, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 170, Loss: 0.01270,Test Acc: 0.96354\n",
      "Test: TP: 0178, FP: 0002, TN: 0192, FN: 0012, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 171, Loss: 0.00831,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 172, Loss: 0.01690,Test Acc: 0.96354\n",
      "Test: TP: 0180, FP: 0004, TN: 0190, FN: 0010, Pred_good: 0200, Pred_bad: 0184\n",
      "Epoch: 173, Loss: 0.03468,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 174, Loss: 0.01338,Test Acc: 0.97396\n",
      "Test: TP: 0180, FP: 0000, TN: 0194, FN: 0010, Pred_good: 0204, Pred_bad: 0180\n",
      "Epoch: 175, Loss: 0.00694,Test Acc: 0.97656\n",
      "Test: TP: 0181, FP: 0000, TN: 0194, FN: 0009, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 176, Loss: 0.01355,Test Acc: 0.96615\n",
      "Test: TP: 0177, FP: 0000, TN: 0194, FN: 0013, Pred_good: 0207, Pred_bad: 0177\n",
      "Epoch: 177, Loss: 0.03213,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 178, Loss: 0.01293,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 179, Loss: 0.00938,Test Acc: 0.97135\n",
      "Test: TP: 0181, FP: 0002, TN: 0192, FN: 0009, Pred_good: 0201, Pred_bad: 0183\n",
      "Epoch: 180, Loss: 0.02673,Test Acc: 0.97917\n",
      "Test: TP: 0182, FP: 0000, TN: 0194, FN: 0008, Pred_good: 0202, Pred_bad: 0182\n",
      "Epoch: 181, Loss: 0.01201,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 182, Loss: 0.00595,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 183, Loss: 0.00453,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 184, Loss: 0.00467,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 185, Loss: 0.00331,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 186, Loss: 0.00321,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 187, Loss: 0.01404,Test Acc: 0.96875\n",
      "Test: TP: 0178, FP: 0000, TN: 0194, FN: 0012, Pred_good: 0206, Pred_bad: 0178\n",
      "Epoch: 188, Loss: 0.01769,Test Acc: 0.97135\n",
      "Test: TP: 0179, FP: 0000, TN: 0194, FN: 0011, Pred_good: 0205, Pred_bad: 0179\n",
      "Epoch: 189, Loss: 0.00912,Test Acc: 0.97917\n",
      "Test: TP: 0182, FP: 0000, TN: 0194, FN: 0008, Pred_good: 0202, Pred_bad: 0182\n",
      "Epoch: 190, Loss: 0.00834,Test Acc: 0.97135\n",
      "Test: TP: 0181, FP: 0002, TN: 0192, FN: 0009, Pred_good: 0201, Pred_bad: 0183\n",
      "Epoch: 191, Loss: 0.00833,Test Acc: 0.98177\n",
      "Test: TP: 0183, FP: 0000, TN: 0194, FN: 0007, Pred_good: 0201, Pred_bad: 0183\n",
      "Epoch: 192, Loss: 0.00742,Test Acc: 0.97917\n",
      "Test: TP: 0183, FP: 0001, TN: 0193, FN: 0007, Pred_good: 0200, Pred_bad: 0184\n",
      "Epoch: 193, Loss: 0.00749,Test Acc: 0.97656\n",
      "Test: TP: 0181, FP: 0000, TN: 0194, FN: 0009, Pred_good: 0203, Pred_bad: 0181\n",
      "Epoch: 194, Loss: 0.00582,Test Acc: 0.98177\n",
      "Test: TP: 0183, FP: 0000, TN: 0194, FN: 0007, Pred_good: 0201, Pred_bad: 0183\n",
      "Epoch: 195, Loss: 0.01228,Test Acc: 0.97917\n",
      "Test: TP: 0183, FP: 0001, TN: 0193, FN: 0007, Pred_good: 0200, Pred_bad: 0184\n",
      "Epoch: 196, Loss: 0.01300,Test Acc: 0.98177\n",
      "Test: TP: 0183, FP: 0000, TN: 0194, FN: 0007, Pred_good: 0201, Pred_bad: 0183\n",
      "Epoch: 197, Loss: 0.00270,Test Acc: 0.98177\n",
      "Test: TP: 0183, FP: 0000, TN: 0194, FN: 0007, Pred_good: 0201, Pred_bad: 0183\n",
      "Epoch: 198, Loss: 0.00401,Test Acc: 0.97917\n",
      "Test: TP: 0184, FP: 0002, TN: 0192, FN: 0006, Pred_good: 0198, Pred_bad: 0186\n",
      "Epoch: 199, Loss: 0.01341,Test Acc: 0.96094\n",
      "Test: TP: 0175, FP: 0000, TN: 0194, FN: 0015, Pred_good: 0209, Pred_bad: 0175\n",
      "train & test time span: 1070.7433834075928\n"
     ]
    }
   ],
   "source": [
    "# train & test\n",
    "time_start = time.time()\n",
    "\n",
    "train_loss_a = np.zeros(200)\n",
    "test_acc_a = np.zeros(200)\n",
    "for epoch in range(0,200):\n",
    "    loss = train()\n",
    "    train_acc, tp_, fp_, tn_, fn_, pred_good_, pred_bad_ = test(train_loader)\n",
    "    test_acc, tp_, fp_, tn_, fn_, pred_good_, pred_bad_ = test(test_loader)\n",
    "    train_loss_a[epoch] = loss\n",
    "    test_acc_a[epoch] = test_acc\n",
    "    print('Epoch: {:03d}, Loss: {:.5f},Test Acc: {:.5f}'.\n",
    "          format(epoch, loss, test_acc, ))\n",
    "    print('Test: TP: {:04d}, FP: {:04d}, TN: {:04d}, FN: {:04d}, Pred_good: {:04d}, Pred_bad: {:04d}'.\n",
    "          format(tp_, fp_, tn_, fn_, pred_good_, pred_bad_))\n",
    "\n",
    "print(\"train & test time span:\", time.time() - time_start)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
